# Benchmark configuration using Shampoo optimizer for dense parameters
# Shampoo is a second-order optimizer that can improve convergence
# runs on 2 ranks with Shampoo optimizer for dense layers
# Note: Sparse embeddings use EXACT_ROWWISE_ADAGRAD (Shampoo not supported in fbgemm_gpu)
RunOptions:
  world_size: 2
  num_batches: 10
  num_benchmarks: 1
  num_profiles: 1
  sharding_type: table_wise
  profile_dir: "."
  name: "sparse_data_dist_shampoo"
  # export_stacks: True # enable this to export stack traces
  loglevel: "info"
  dense_optimizer: "Shampoo"
  dense_lr: 0.01
  sparse_optimizer: "EXACT_ROWWISE_ADAGRAD"
  sparse_lr: 0.01
PipelineConfig:
  pipeline: "sparse"
ModelInputConfig:
  num_float_features: 100
  feature_pooling_avg: 30
ModelSelectionConfig:
  model_name: "test_sparse_nn"
  model_config:
    num_float_features: 100
    submodule_kwargs:
      dense_arch_out_size: 128
      over_arch_out_size: 1024
      over_arch_hidden_layers: 5
      dense_arch_hidden_sizes: [128, 128, 128]
EmbeddingTablesConfig:
  num_unweighted_features: 90
  num_weighted_features: 80
  embedding_feature_dim: 256
  additional_tables:
    - - name: FP16_table
        embedding_dim: 512
        num_embeddings: 100_000
        feature_names: ["additional_0_0"]
        data_type: FP16
      - name: large_table
        embedding_dim: 2048
        num_embeddings: 1_000_000
        feature_names: ["additional_0_1"]
    - []
    - - name: skipped_table
        embedding_dim: 128
        num_embeddings: 100_000
        feature_names: ["additional_2_1"]
PlannerConfig:
  additional_constraints:
    large_table:
      sharding_types: [column_wise]
