# Running TorchRec with TorchX using Ray Scheduler

This example demonstrates how to run **distributed TorchRec training on a Ray cluster** using TorchX as the job launcher. Ray provides dynamic resource allocation and scaling for distributed ML workloads.

## Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        TORCHX + RAY DISTRIBUTED TRAINING                                 â”‚
â”‚                                                                                          â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚                              YOUR MACHINE                                        â”‚   â”‚
â”‚   â”‚                                                                                  â”‚   â”‚
â”‚   â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚   â”‚
â”‚   â”‚   â”‚  torchx run -s ray ...                                                  â”‚   â”‚   â”‚
â”‚   â”‚   â”‚                                                                         â”‚   â”‚   â”‚
â”‚   â”‚   â”‚  TorchX handles:                                                        â”‚   â”‚   â”‚
â”‚   â”‚   â”‚  â€¢ Job submission to Ray cluster                                        â”‚   â”‚   â”‚
â”‚   â”‚   â”‚  â€¢ Resource allocation (GPUs, memory)                                   â”‚   â”‚   â”‚
â”‚   â”‚   â”‚  â€¢ Environment setup (working_dir, requirements.txt)                    â”‚   â”‚   â”‚
â”‚   â”‚   â”‚  â€¢ Process group initialization for distributed training                â”‚   â”‚   â”‚
â”‚   â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚   â”‚
â”‚   â”‚                                    â”‚                                             â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                        â”‚                                                 â”‚
â”‚                                        â–¼                                                 â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚                              RAY CLUSTER                                         â”‚   â”‚
â”‚   â”‚                                                                                  â”‚   â”‚
â”‚   â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                   â”‚   â”‚
â”‚   â”‚   â”‚      HEAD NODE          â”‚                                                   â”‚   â”‚
â”‚   â”‚   â”‚   (Ray Dashboard)       â”‚                                                   â”‚   â”‚
â”‚   â”‚   â”‚   localhost:6379        â”‚                                                   â”‚   â”‚
â”‚   â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                                   â”‚   â”‚
â”‚   â”‚               â”‚                                                                  â”‚   â”‚
â”‚   â”‚               â”‚  Coordinates job scheduling                                      â”‚   â”‚
â”‚   â”‚               â”‚                                                                  â”‚   â”‚
â”‚   â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚   â”‚
â”‚   â”‚   â”‚                                                                        â”‚     â”‚   â”‚
â”‚   â”‚   â–¼                          â–¼                          â–¼                  â”‚     â”‚   â”‚
â”‚   â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚     â”‚   â”‚
â”‚   â”‚   â”‚  WORKER NODE 0  â”‚      â”‚  WORKER NODE 1  â”‚      â”‚  WORKER NODE N  â”‚   â”‚     â”‚   â”‚
â”‚   â”‚   â”‚                 â”‚      â”‚                 â”‚      â”‚                 â”‚   â”‚     â”‚   â”‚
â”‚   â”‚   â”‚  GPU 0   GPU 1  â”‚      â”‚  GPU 0   GPU 1  â”‚      â”‚  GPU 0   GPU 1  â”‚   â”‚     â”‚   â”‚
â”‚   â”‚   â”‚  â”Œâ”€â”€â”€â”   â”Œâ”€â”€â”€â”  â”‚      â”‚  â”Œâ”€â”€â”€â”   â”Œâ”€â”€â”€â”  â”‚      â”‚  â”Œâ”€â”€â”€â”   â”Œâ”€â”€â”€â”  â”‚   â”‚     â”‚   â”‚
â”‚   â”‚   â”‚  â”‚P0 â”‚   â”‚P1 â”‚  â”‚      â”‚  â”‚P2 â”‚   â”‚P3 â”‚  â”‚      â”‚  â”‚P4 â”‚   â”‚P5 â”‚  â”‚   â”‚     â”‚   â”‚
â”‚   â”‚   â”‚  â””â”€â”€â”€â”˜   â””â”€â”€â”€â”˜  â”‚      â”‚  â””â”€â”€â”€â”˜   â””â”€â”€â”€â”˜  â”‚      â”‚  â””â”€â”€â”€â”˜   â””â”€â”€â”€â”˜  â”‚   â”‚     â”‚   â”‚
â”‚   â”‚   â”‚                 â”‚      â”‚                 â”‚      â”‚                 â”‚   â”‚     â”‚   â”‚
â”‚   â”‚   â”‚  TorchRec DMP   â”‚      â”‚  TorchRec DMP   â”‚      â”‚  TorchRec DMP   â”‚   â”‚     â”‚   â”‚
â”‚   â”‚   â”‚  (Shard 0-1)    â”‚      â”‚  (Shard 2-3)    â”‚      â”‚  (Shard 4-5)    â”‚   â”‚     â”‚   â”‚
â”‚   â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚     â”‚   â”‚
â”‚   â”‚                                                                            â”‚     â”‚   â”‚
â”‚   â”‚   P0-P5 = Training processes with NCCL backend for GPU communication       â”‚     â”‚   â”‚
â”‚   â”‚                                                                            â”‚     â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚   â”‚
â”‚                                                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Training Flow on Ray

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         DISTRIBUTED TRAINING FLOW                                        â”‚
â”‚                                                                                          â”‚
â”‚   PHASE 1: Job Submission                                                                â”‚
â”‚   â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•                                                                â”‚
â”‚                                                                                          â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚  torchx run -s ray \                                                             â”‚   â”‚
â”‚   â”‚      -cfg dashboard_address=localhost:6379,working_dir=./,requirements=./req.txt â”‚   â”‚
â”‚   â”‚      dist.ddp -j 1x2 --gpu 2 \                                                   â”‚   â”‚
â”‚   â”‚      --script train_torchrec.py                                                  â”‚   â”‚
â”‚   â”‚                                                                                  â”‚   â”‚
â”‚   â”‚  TorchX parses arguments:                                                        â”‚   â”‚
â”‚   â”‚  â€¢ -j 1x2: 1 job with 2 workers                                                  â”‚   â”‚
â”‚   â”‚  â€¢ --gpu 2: 2 GPUs per worker                                                    â”‚   â”‚
â”‚   â”‚  â€¢ --script: Python script to run                                                â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                          â”‚                                               â”‚
â”‚                                          â–¼                                               â”‚
â”‚   PHASE 2: Ray Scheduling                                                                â”‚
â”‚   â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•                                                                â”‚
â”‚                                                                                          â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚  Ray Scheduler:                                                                  â”‚   â”‚
â”‚   â”‚                                                                                  â”‚   â”‚
â”‚   â”‚  1. Finds nodes with available GPUs                                              â”‚   â”‚
â”‚   â”‚  2. Allocates resources for each worker                                          â”‚   â”‚
â”‚   â”‚  3. Copies working_dir to workers                                                â”‚   â”‚
â”‚   â”‚  4. Installs requirements.txt dependencies                                       â”‚   â”‚
â”‚   â”‚  5. Sets environment variables (RANK, WORLD_SIZE, MASTER_ADDR, etc.)            â”‚   â”‚
â”‚   â”‚  6. Launches training processes                                                  â”‚   â”‚
â”‚   â”‚                                                                                  â”‚   â”‚
â”‚   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚   â”‚
â”‚   â”‚  â”‚  Worker 0:  RANK=0, LOCAL_RANK=0, WORLD_SIZE=2                          â”‚    â”‚   â”‚
â”‚   â”‚  â”‚  Worker 1:  RANK=1, LOCAL_RANK=1, WORLD_SIZE=2                          â”‚    â”‚   â”‚
â”‚   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚   â”‚
â”‚   â”‚                                                                                  â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                          â”‚                                               â”‚
â”‚                                          â–¼                                               â”‚
â”‚   PHASE 3: TorchRec Distributed Training                                                 â”‚
â”‚   â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•                                                 â”‚
â”‚                                                                                          â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚                                                                                  â”‚   â”‚
â”‚   â”‚   Each worker runs train_torchrec.py:                                            â”‚   â”‚
â”‚   â”‚                                                                                  â”‚   â”‚
â”‚   â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚   â”‚
â”‚   â”‚   â”‚  # Initialize distributed                                               â”‚   â”‚   â”‚
â”‚   â”‚   â”‚  dist.init_process_group(backend="nccl")                                â”‚   â”‚   â”‚
â”‚   â”‚   â”‚                                                                         â”‚   â”‚   â”‚
â”‚   â”‚   â”‚  # Create model with TorchRec                                           â”‚   â”‚   â”‚
â”‚   â”‚   â”‚  model = DLRM(...)                                                      â”‚   â”‚   â”‚
â”‚   â”‚   â”‚                                                                         â”‚   â”‚   â”‚
â”‚   â”‚   â”‚  # Wrap with DistributedModelParallel                                   â”‚   â”‚   â”‚
â”‚   â”‚   â”‚  model = DistributedModelParallel(model, ...)                           â”‚   â”‚   â”‚
â”‚   â”‚   â”‚                                                                         â”‚   â”‚   â”‚
â”‚   â”‚   â”‚  # Training loop                                                        â”‚   â”‚   â”‚
â”‚   â”‚   â”‚  for batch in dataloader:                                               â”‚   â”‚   â”‚
â”‚   â”‚   â”‚      loss = model(batch)                                                â”‚   â”‚   â”‚
â”‚   â”‚   â”‚      loss.backward()                                                    â”‚   â”‚   â”‚
â”‚   â”‚   â”‚      optimizer.step()                                                   â”‚   â”‚   â”‚
â”‚   â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚   â”‚
â”‚   â”‚                                                                                  â”‚   â”‚
â”‚   â”‚   Communication Pattern:                                                         â”‚   â”‚
â”‚   â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚   â”‚
â”‚   â”‚   â”‚  Worker 0 â—„â”€â”€â”€â”€â”€ All-to-All (NCCL) â”€â”€â”€â”€â”€â–º Worker 1                      â”‚   â”‚   â”‚
â”‚   â”‚   â”‚     â”‚                                          â”‚                        â”‚   â”‚   â”‚
â”‚   â”‚   â”‚     â”‚  Embedding shards exchanged during       â”‚                        â”‚   â”‚   â”‚
â”‚   â”‚   â”‚     â”‚  forward and backward passes             â”‚                        â”‚   â”‚   â”‚
â”‚   â”‚   â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                        â”‚   â”‚   â”‚
â”‚   â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚   â”‚
â”‚   â”‚                                                                                  â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Job Management Visualization

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                              JOB LIFECYCLE                                               â”‚
â”‚                                                                                          â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚                                                                                  â”‚   â”‚
â”‚   â”‚   SUBMIT                    RUNNING                    COMPLETE                  â”‚   â”‚
â”‚   â”‚   â”Œâ”€â”€â”€â”€â”€â”                  â”Œâ”€â”€â”€â”€â”€â”                    â”Œâ”€â”€â”€â”€â”€â”                    â”‚   â”‚
â”‚   â”‚   â”‚     â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º  â”‚     â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º  â”‚     â”‚                    â”‚   â”‚
â”‚   â”‚   â”‚ â³  â”‚    scheduling    â”‚ ğŸ”„  â”‚    training       â”‚ âœ…  â”‚                    â”‚   â”‚
â”‚   â”‚   â”‚     â”‚                  â”‚     â”‚                    â”‚     â”‚                    â”‚   â”‚
â”‚   â”‚   â””â”€â”€â”€â”€â”€â”˜                  â””â”€â”€â”€â”€â”€â”˜                    â””â”€â”€â”€â”€â”€â”˜                    â”‚   â”‚
â”‚   â”‚      â”‚                        â”‚                          â”‚                       â”‚   â”‚
â”‚   â”‚      â”‚                        â”‚                          â”‚                       â”‚   â”‚
â”‚   â”‚      â–¼                        â–¼                          â–¼                       â”‚   â”‚
â”‚   â”‚   PENDING                  RUNNING                   SUCCEEDED                   â”‚   â”‚
â”‚   â”‚                            FAILED â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º âŒ                         â”‚   â”‚
â”‚   â”‚                            INTERRUPTED â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º âš ï¸                         â”‚   â”‚
â”‚   â”‚                                                                                  â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                                          â”‚
â”‚   Job ID Format: ray://torchx/<dashboard_address>-<job_id>                              â”‚
â”‚   Example: ray://torchx/172.31.16.248:6379-raysubmit_ntquG1dDV6CtFUC5                   â”‚
â”‚                                                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Installation

```bash
pip install --pre torchrec -f https://download.pytorch.org/whl/torchrec/index.html
pip install torchx-nightly
pip install "ray[default]" -qqq
```

## Running the Example

### On a Ray Cluster

Run torchx with the dashboard address and a link to your component:

```bash
torchx run -s ray \
    -cfg dashboard_address=localhost:6379,working_dir=~/repos/torchrec/examples/ray,requirements=./requirements.txt \
    dist.ddp -j 1x2 \
    --script ~/repos/torchrec/examples/ray/train_torchrec.py
```

### Run Locally (without Ray cluster)

```bash
torchx run -s ray \
    -cfg working_dir=~/repos/torchrec/examples/ray,requirements=./requirements.txt \
    dist.ddp -j 1x2 \
    --script ~/repos/torchrec/examples/ray/train_torchrec.py
```

### With GPUs

To run with GPUs, add `--gpu` flag to the dist.ddp component:

```bash
torchx run -s ray \
    -cfg working_dir=~/repos/torchrec/examples/ray,requirements=./requirements.txt \
    dist.ddp -j 1x2 --gpu 2 \
    --script ~/repos/torchrec/examples/ray/train_torchrec.py
```

For other dist.ddp options, see: https://pytorch.org/torchx/latest/components/distributed.html

### Without Ray Scheduler (TorchX only)

```bash
torchx run -s local_cwd dist.ddp -j 1x2 --script ~/repos/torchrec/examples/ray/train_torchrec.py
```

For available settings: https://pytorch.org/torchx/latest/cli.html

## Job Management

Job ID format: `ray://torchx/<dashboard_address>-<job_id>`

Example: `ray://torchx/172.31.16.248:6379-raysubmit_ntquG1dDV6CtFUC5`

### Check Job Status

```bash
torchx status ray://torchx/172.31.16.248:6379-raysubmit_ntquG1dDV6CtFUC5
```

Possible statuses: `PENDING`, `RUNNING`, `SUCCEEDED`, `FAILED`, `INTERRUPTED`

### View Logs

```bash
torchx log ray://torchx/172.31.16.248:6379-raysubmit_ntquG1dDV6CtFUC5/worker/0
```

## Directory Structure

```
ray/
â”œâ”€â”€ README.md               # This file
â”œâ”€â”€ __init__.py
â”œâ”€â”€ compute_world_size.py   # Utility for distributed world size
â”œâ”€â”€ requirements.txt        # Python dependencies
â””â”€â”€ train_torchrec.py       # Main training script
```

## References

- [TorchX Documentation](https://pytorch.org/torchx/)
- [Ray Documentation](https://docs.ray.io/)
- [TorchX Distributed Components](https://pytorch.org/torchx/latest/components/distributed.html)
- [TorchRec Documentation](https://pytorch.org/torchrec/)
