
<!DOCTYPE html>


<html lang="en" data-content_root="./" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />
<meta content="TorchRec Concepts" name="description" />
<meta content="recommendation systems, sharding, distributed training, torchrec, embedding bags, embeddings, keyedjaggedtensor, row wise, table wise, column wise, table row wise, planner, sharder" name="keywords" />

    <title>TorchRec Concepts &#8212; TorchRec 1.0.0 documentation</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=8f2a1f02" />
    <link rel="stylesheet" type="text/css" href="_static/css/theme.css?v=36fba2ff" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="_static/documentation_options.js?v=8d563738"></script>
    <script src="_static/doctools.js?v=888ff710"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="_static/copybutton.js?v=f281be69"></script>
    <script src="_static/design-tabs.js?v=f930bc37"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'concepts';</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Setup" href="setup-torchrec.html" />
    <link rel="prev" title="TorchRec High Level Architecture" href="high-level-arch.html" />

  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
<script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/2.3.1/list.min.js"></script>
<script>
  if (window.location.hostname === 'docs.pytorch.org' || window.location.hostname === 'docs-preview.pytorch.org') {
    const script = document.createElement('script');
    script.src = 'https://cmp.osano.com/16A0DbT9yDNIaQkvZ/31b1b91a-e0b6-47ea-bde2-7f2bd13dbe5c/osano.js?variant=one';
    document.head.appendChild(script);
  }
</script>
<script>
  // Cookie banner for non-LF projects
  document.addEventListener('DOMContentLoaded', function () {
    // Hide cookie banner on local environments and LF owned docs
    if (window.location.hostname === 'localhost' ||
      window.location.hostname === '0.0.0.0' ||
      window.location.hostname === '127.0.0.1' ||
      window.location.hostname === 'docs.pytorch.org' ||
      window.location.hostname === 'docs-preview.pytorch.org' ||
      window.location.hostname.startsWith('192.168.')) {
      const banner = document.querySelector('.cookie-banner-wrapper');
      if (banner) {
        banner.style.display = 'none';
      }
    }
  });
</script>
<!-- Conditional CSS for header and footer height adjustment -->

<style>
  :root {
    --header-height: 0px !important;
    --header-height-desktop: 0px !important;
  }
</style>


<style>
  @media (min-width: 1100px) {
    .site-footer {
      height: 300px !important;
    }
  }
</style>

<link rel="stylesheet" type="text/css" href="_static/css/theme.css" crossorigin="anonymous">
<script type="text/javascript" src="_static/js/theme.js"></script>
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Montserrat:wght@300;400&display=swap" rel="stylesheet">
<meta property="og:image" content="https://docs.pytorch.org/docs/stable/_static/img/pytorch_seo.png" />
<link rel="stylesheet" href="_static/webfonts/all.min.css" crossorigin="anonymous">
<meta http-equiv="Content-Security-Policy"
  content="default-src * 'unsafe-inline' 'unsafe-eval' data: blob:; style-src * 'unsafe-inline'; script-src * 'unsafe-inline' 'unsafe-eval' blob:;">
<meta name="pytorch_project" content="">
<!-- Google Tag Manager (noscript) -->
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-NPLPKN5G" height="0" width="0"
    style="display:none;visibility:hidden"></iframe></noscript>
<!-- End Google Tag Manager (noscript) -->
<!-- Google Tag Manager -->
<script>(function (w, d, s, l, i) {
    w[l] = w[l] || []; w[l].push({
      'gtm.start':
        new Date().getTime(), event: 'gtm.js'
    }); var f = d.getElementsByTagName(s)[0],
      j = d.createElement(s), dl = l != 'dataLayer' ? '&l=' + l : ''; j.async = true; j.src =
        'https://www.googletagmanager.com/gtm.js?id=' + i + dl; f.parentNode.insertBefore(j, f);
    j.onload = function () {
      window.dispatchEvent(new Event('gtm_loaded'));
      console.log('GTM loaded successfully');
    };
  })(window, document, 'script', 'dataLayer', 'GTM-NPLPKN5G');
</script>
<!-- End Google Tag Manager -->
<!-- Facebook Pixel Code -->
<script>
  !function (f, b, e, v, n, t, s) {
    if (f.fbq) return; n = f.fbq = function () {
      n.callMethod ?
        n.callMethod.apply(n, arguments) : n.queue.push(arguments)
    };
    if (!f._fbq) f._fbq = n; n.push = n; n.loaded = !0; n.version = '2.0';
    n.queue = []; t = b.createElement(e); t.async = !0;
    t.src = v; s = b.getElementsByTagName(e)[0];
    s.parentNode.insertBefore(t, s)
  }(window, document, 'script',
    'https://connect.facebook.net/en_US/fbevents.js');
  fbq('init', '243028289693773');
  fbq('track', 'PageView');
</script>
<script>
  document.documentElement.setAttribute('data-version', '1.0.0');
</script>
<noscript>
  <img height="1" width="1" src="https://www.facebook.com/tr?id=243028289693773&ev=PageView&noscript=1" />
</noscript>
<script>
  function gtag() {
    window.dataLayer.push(arguments);
  }
</script>
<!-- End Facebook Pixel Code -->
<!-- Repository configuration for tutorials -->

<!-- Script to Fix scrolling -->
<script>
  document.addEventListener('DOMContentLoaded', function () {
    // Fix anchor scrolling
    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
      anchor.addEventListener('click', function (e) {
        e.preventDefault();
        const targetId = this.getAttribute('href').substring(1);
        const targetElement = document.getElementById(targetId);

        if (targetElement) {
          const headerHeight =
            (document.querySelector('.header-holder') ? document.querySelector('.header-holder').offsetHeight : 0) +
            (document.querySelector('.bd-header') ? document.querySelector('.bd-header').offsetHeight : 0) + 20;

          const targetPosition = targetElement.getBoundingClientRect().top + window.pageYOffset - headerHeight;
          window.scrollTo({
            top: targetPosition,
            behavior: 'smooth'
          });

          // Update URL hash without scrolling
          history.pushState(null, null, '#' + targetId);
        }
      });
    });
  });
</script>

<script async src="https://cse.google.com/cse.js?cx=e65585f8c3ea1440e"></script>

  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>

  </head>

<body data-feedback-url="https://github.com/meta-pytorch/torchrec" class="pytorch-body">
  
  
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
<div class="bd-header__inner bd-page-width">
  <button class="pst-navbar-icon sidebar-toggle primary-toggle" aria-label="Site navigation">
    <span class="fa-solid fa-bars"></span>
  </button>
  
  
  <div class=" navbar-header-items__start">
    
      <div class="navbar-item">

  
    
  

<a class="navbar-brand logo" href="index.html">
  
  
  
  
  
  
    <p class="title logo__title">Home</p>
  
</a></div>
    
  </div>
  
  <div class=" navbar-header-items">
    
    <div class="me-auto navbar-header-items__center">
      
        <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item current active">
  <a class="nav-link nav-internal" href="intro.html">
    Intro
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="setup-torchrec.html">
    Setup
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="api.html">
    API reference
  </a>
</li>

  </ul>
</nav></div>
      
    </div>
    
    
    <div class="navbar-header-items__end">
      
      
        <div class="navbar-item">
  
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
</div>
      
        <div class="navbar-item">

<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script></div>
      
        <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://x.com/PyTorch" title="X" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-x-twitter fa-lg" aria-hidden="true"></i>
            <span class="sr-only">X</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/meta-pytorch/torchrec" title="GitHub" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">GitHub</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://dev-discuss.pytorch.org/" title="Discourse" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-discourse fa-lg" aria-hidden="true"></i>
            <span class="sr-only">Discourse</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://pypi.org/project/torchrec/" title="PyPi" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-python fa-lg" aria-hidden="true"></i>
            <span class="sr-only">PyPi</span></a>
        </li>
</ul></div>
      
    </div>
    
  </div>
  
  

  
    <button class="pst-navbar-icon sidebar-toggle secondary-toggle" aria-label="On this page">
      <span class="fa-solid fa-outdent"></span>
    </button>
  
</div>

    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
      <div class="sidebar-header-items__center">
        
          
          
            <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item current active">
  <a class="nav-link nav-internal" href="intro.html">
    Intro
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="setup-torchrec.html">
    Setup
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="api.html">
    API reference
  </a>
</li>

  </ul>
</nav></div>
          
        
      </div>
    
    
    
      <div class="sidebar-header-items__end">
        
          <div class="navbar-item">
  
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
</div>
        
          <div class="navbar-item">

<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script></div>
        
          <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://x.com/PyTorch" title="X" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-x-twitter fa-lg" aria-hidden="true"></i>
            <span class="sr-only">X</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/meta-pytorch/torchrec" title="GitHub" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">GitHub</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://dev-discuss.pytorch.org/" title="Discourse" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-discourse fa-lg" aria-hidden="true"></i>
            <span class="sr-only">Discourse</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://pypi.org/project/torchrec/" title="PyPi" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-python fa-lg" aria-hidden="true"></i>
            <span class="sr-only">PyPi</span></a>
        </li>
</ul></div>
        
      </div>
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
<nav class="bd-docs-nav bd-links"
     aria-label="Section Navigation">
  <p class="bd-links__title" role="heading" aria-level="1">Section Navigation</p>
  <div class="bd-toc-item navbar-nav"><ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="overview.html">TorchRec Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="high-level-arch.html">TorchRec High Level Architecture</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">TorchRec Concepts</a></li>
</ul>
</div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        
          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item">



<nav aria-label="Breadcrumb" class="d-print-none">
  <ul class="bd-breadcrumbs">
    
    <li class="breadcrumb-item breadcrumb-home">
      <a href="index.html" class="nav-link" aria-label="Home">
        <i class="fa-solid fa-home"></i>
      </a>
    </li>
    
    <li class="breadcrumb-item"><a href="intro.html" class="nav-link">Intro</a></li>
    
    <li class="breadcrumb-item active" aria-current="page">TorchRec Concepts</li>
  </ul>
</nav>
</div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">
<div class="rating">
    Rate this Page
    <div class="stars">
        
        <span class="star" data-behavior="tutorial-rating" data-count="1" data-value="1">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="2" data-value="2">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="3" data-value="3">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="4" data-value="4">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="5" data-value="5">★</span>
        
    </div>
</div>
</div>
      
    </div>
  
</div>
</div>
              
              
  
<div id="searchbox"></div>
  <article class="bd-article" id="pytorch-article">
    <!-- Hidden breadcrumb schema for SEO only -->
    <div style="display:none;" itemscope itemtype="https://schema.org/BreadcrumbList">
      
      <div itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
        <link itemprop="item" href="intro.html">
        <meta itemprop="name" content="Intro">
        <meta itemprop="position" content="1">
      </div>
      
      <div itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
        <meta itemprop="name" content="TorchRec Concepts">
        <meta itemprop="position" content="2">
      </div>
    </div>

    
    

    
              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section id="torchrec-concepts">
<h1>TorchRec Concepts<a class="headerlink" href="#torchrec-concepts" title="Link to this heading">#</a></h1>
<p>In this section, we will learn about the key concepts of TorchRec,
designed to optimize large-scale recommendation systems using PyTorch.
We will learn how each concept works in detail and review how it is used
with the rest of TorchRec.</p>
<p>TorchRec has specific input/output data types of its modules to
efficiently represent sparse features, including:</p>
<ul class="simple">
<li><p><strong>JaggedTensor:</strong> a wrapper around the lengths/offsets and values
tensors for a singular sparse feature.</p></li>
<li><p><strong>KeyedJaggedTensor:</strong> efficiently represent multiple sparse
features, can think of it as multiple <code class="docutils literal notranslate"><span class="pre">JaggedTensor</span></code>s.</p></li>
<li><p><strong>KeyedTensor:</strong> a wrapper around <code class="docutils literal notranslate"><span class="pre">torch.Tensor</span></code> that allows access
to tensor values through keys.</p></li>
</ul>
<p>With the goal of high performance and efficiency, the canonical
<code class="docutils literal notranslate"><span class="pre">torch.Tensor</span></code> is highly inefficient for representing sparse data.
TorchRec introduces these new data types because they provide efficient
storage and representation of sparse input data. As you will see later
on, the <code class="docutils literal notranslate"><span class="pre">KeyedJaggedTensor</span></code> makes communication of input data in a
distributed environment very efficient leading to one of the key
performance advantages that TorchRec provides.</p>
<p>In the end-to-end training loop, TorchRec comprises of the following
main components:</p>
<ul class="simple">
<li><p><strong>Planner:</strong> Takes in the configuration of embedding tables,
environment setup, and generates an optimized sharding plan for the
model.</p></li>
<li><p><strong>Sharder:</strong> Shards model according to sharding plan with different
sharding strategies including data-parallel, table-wise, row-wise,
table-wise-row-wise, column-wise, and table-wise-column-wise
sharding.</p></li>
<li><p><strong>DistributedModelParallel:</strong> Combines sharder, optimizer, and
provides an entry point into the training the model in a distributed
manner.</p></li>
</ul>
<section id="jaggedtensor">
<h2>JaggedTensor<a class="headerlink" href="#jaggedtensor" title="Link to this heading">#</a></h2>
<p>A <code class="docutils literal notranslate"><span class="pre">JaggedTensor</span></code> represents a sparse feature through lengths, values,
and offsets. It is called “jagged” because it efficiently represents
data with variable-length sequences. In contrast, a canonical
<code class="docutils literal notranslate"><span class="pre">torch.Tensor</span></code> assumes that each sequence has the same length, which
is often not the case with real world data. A <code class="docutils literal notranslate"><span class="pre">JaggedTensor</span></code>
facilitates the representation of such data without padding making it
highly efficient.</p>
<p>Key Components:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">Lengths</span></code>: A list of integers representing the number of elements
for each entity.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Offsets</span></code>: A list of integers representing the starting index of
each sequence in the flattened values tensor. These provide an
alternative to lengths.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Values</span></code>: A 1D tensor containing the actual values for each entity,
stored contiguously.</p></li>
</ul>
<p>Here is a simple example demonstrating how each of the components would
look like:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># User interactions:</span>
<span class="c1"># - User 1 interacted with 2 items</span>
<span class="c1"># - User 2 interacted with 3 items</span>
<span class="c1"># - User 3 interacted with 1 item</span>
<span class="n">lengths</span> <span class="o">=</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="n">offsets</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">]</span>  <span class="c1"># Starting index of each user&#39;s interactions</span>
<span class="n">values</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([</span><span class="mi">101</span><span class="p">,</span> <span class="mi">102</span><span class="p">,</span> <span class="mi">201</span><span class="p">,</span> <span class="mi">202</span><span class="p">,</span> <span class="mi">203</span><span class="p">,</span> <span class="mi">301</span><span class="p">])</span>  <span class="c1"># Item IDs interacted with</span>
<span class="n">jt</span> <span class="o">=</span> <span class="n">JaggedTensor</span><span class="p">(</span><span class="n">lengths</span><span class="o">=</span><span class="n">lengths</span><span class="p">,</span> <span class="n">values</span><span class="o">=</span><span class="n">values</span><span class="p">)</span>
<span class="c1"># OR</span>
<span class="n">jt</span> <span class="o">=</span> <span class="n">JaggedTensor</span><span class="p">(</span><span class="n">offsets</span><span class="o">=</span><span class="n">offsets</span><span class="p">,</span> <span class="n">values</span><span class="o">=</span><span class="n">values</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="keyedjaggedtensor">
<h2>KeyedJaggedTensor<a class="headerlink" href="#keyedjaggedtensor" title="Link to this heading">#</a></h2>
<p>A <code class="docutils literal notranslate"><span class="pre">KeyedJaggedTensor</span></code> extends the functionality of <code class="docutils literal notranslate"><span class="pre">JaggedTensor</span></code> by
introducing keys (which are typically feature names) to label different
groups of features, for example, user features and item features. This
is the data type used in <code class="docutils literal notranslate"><span class="pre">forward</span></code> of <code class="docutils literal notranslate"><span class="pre">EmbeddingBagCollection</span></code> and
<code class="docutils literal notranslate"><span class="pre">EmbeddingCollection</span></code> as they are used to represent multiple features
in a table.</p>
<p>A <code class="docutils literal notranslate"><span class="pre">KeyedJaggedTensor</span></code> has an implied batch size, which is the length
of <code class="docutils literal notranslate"><span class="pre">lengths</span></code> tensor divided by the number of keys. The example
below has a batch size of 2 (4 lengths divided by 2 keys).
Similar to a <code class="docutils literal notranslate"><span class="pre">JaggedTensor</span></code>, the
<code class="docutils literal notranslate"><span class="pre">offsets</span></code> and <code class="docutils literal notranslate"><span class="pre">lengths</span></code> function in the same manner. You can also
access the <code class="docutils literal notranslate"><span class="pre">lengths</span></code>, <code class="docutils literal notranslate"><span class="pre">offsets</span></code>, and <code class="docutils literal notranslate"><span class="pre">values</span></code> of a feature by
accessing the key from the <code class="docutils literal notranslate"><span class="pre">KeyedJaggedTensor</span></code>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">keys</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;user_features&quot;</span><span class="p">,</span> <span class="s2">&quot;item_features&quot;</span><span class="p">]</span>
<span class="c1"># Lengths of interactions:</span>
<span class="c1"># - User features: 2 users, with 2 and 3 interactions respectively</span>
<span class="c1"># - Item features: 2 items, with 1 and 2 interactions respectively</span>
<span class="n">lengths</span> <span class="o">=</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span>
<span class="n">values</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([</span><span class="mi">11</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">21</span><span class="p">,</span> <span class="mi">22</span><span class="p">,</span> <span class="mi">23</span><span class="p">,</span> <span class="mi">101</span><span class="p">,</span> <span class="mi">201</span><span class="p">,</span> <span class="mi">202</span><span class="p">])</span>
<span class="c1"># Create a KeyedJaggedTensor</span>
<span class="n">kjt</span> <span class="o">=</span> <span class="n">KeyedJaggedTensor</span><span class="p">(</span><span class="n">keys</span><span class="o">=</span><span class="n">keys</span><span class="p">,</span> <span class="n">lengths</span><span class="o">=</span><span class="n">lengths</span><span class="p">,</span> <span class="n">values</span><span class="o">=</span><span class="n">values</span><span class="p">)</span>
<span class="c1"># Access the features by key</span>
<span class="nb">print</span><span class="p">(</span><span class="n">kjt</span><span class="p">[</span><span class="s2">&quot;user_features&quot;</span><span class="p">])</span>
<span class="c1"># Outputs user features</span>
<span class="nb">print</span><span class="p">(</span><span class="n">kjt</span><span class="p">[</span><span class="s2">&quot;item_features&quot;</span><span class="p">])</span>
</pre></div>
</div>
</section>
<section id="planner">
<h2>Planner<a class="headerlink" href="#planner" title="Link to this heading">#</a></h2>
<p>The TorchRec planner helps determine the best sharding configuration for
a model. It evaluates multiple possibilities for sharding embedding
tables and optimizes for performance. The planner performs the
following:</p>
<ul class="simple">
<li><p>Assesses the memory constraints of the hardware.</p></li>
<li><p>Estimates compute requirements based on memory fetches, such as
embedding lookups.</p></li>
<li><p>Addresses data-specific factors.</p></li>
<li><p>Considers other hardware specifics, such as bandwidth, to generate an
optimal sharding plan.</p></li>
</ul>
<p>To ensure accurate consideration of these factors, the Planner can
incorporate data about the embedding tables, constraints, hardware
information, and topology to help in generating an optimal plan.</p>
</section>
<section id="sharding-of-embeddingtables">
<h2>Sharding of EmbeddingTables<a class="headerlink" href="#sharding-of-embeddingtables" title="Link to this heading">#</a></h2>
<p>TorchRec sharder provides multiple sharding strategies for various use
cases, we outline some of the sharding strategies and how they work as
well as their benefits and limitations. Generally, we recommend using
the TorchRec planner to generate a sharding plan for you as it will find
the optimal sharding strategy for each embedding table in your model.</p>
<p>Each sharding strategy determines how to do the table split, whether the
table should be cut up and how, whether to keep one or a few copies of
some tables, and so on. Each piece of the table from the outcome of
sharding, whether it is one embedding table or part of it, is referred
to as a shard.</p>
<figure class="align-center" id="id1">
<img alt="Visualizing the difference of sharding types offered in TorchRec" src="_images/sharding.png" />
<figcaption>
<p><span class="caption-text"><em>Figure 1: Visualizing the placement of table shards under different sharding schemes offered in TorchRec</em></span><a class="headerlink" href="#id1" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>Here is the list of all sharding types available in TorchRec:</p>
<ul class="simple">
<li><p>Table-wise (TW): as the name suggests, embedding table is kept as a
whole piece and placed on one rank.</p></li>
<li><p>Column-wise (CW): the table is split along the <code class="docutils literal notranslate"><span class="pre">emb_dim</span></code> dimension,
for example, <code class="docutils literal notranslate"><span class="pre">emb_dim=256</span></code> is split into 4 shards: <code class="docutils literal notranslate"><span class="pre">[64,</span> <span class="pre">64,</span> <span class="pre">64,</span>
<span class="pre">64]</span></code>.</p></li>
<li><p>Row-wise (RW): the table is split along the <code class="docutils literal notranslate"><span class="pre">hash_size</span></code> dimension,
usually split evenly among all the ranks.</p></li>
<li><p>Table-wise-row-wise (TWRW): table is placed on one host, split
row-wise among the ranks on that host.</p></li>
<li><p>Grid-shard (GS): a table is CW sharded and each CW shard is placed
TWRW on a host.</p></li>
<li><p>Data parallel (DP): each rank keeps a copy of the table.</p></li>
</ul>
<p>Once sharded, the modules are converted to sharded versions of
themselves, known as <code class="docutils literal notranslate"><span class="pre">ShardedEmbeddingCollection</span></code> and
<code class="docutils literal notranslate"><span class="pre">ShardedEmbeddingBagCollection</span></code> in TorchRec. These modules handle the
communication of input data, embedding lookups, and gradients.</p>
</section>
<section id="distributed-training-with-torchrec-sharded-modules">
<h2>Distributed Training with TorchRec Sharded Modules<a class="headerlink" href="#distributed-training-with-torchrec-sharded-modules" title="Link to this heading">#</a></h2>
<p>With many sharding strategies available, how do we determine which one
to use? There is a cost associated with each sharding scheme, which in
conjunction with model size and number of GPUs determines which sharding
strategy is best for a model.</p>
<p>Without sharding, where each GPU keeps a copy of the embedding table
(DP), the main cost is computation in which each GPU looks up the
embedding vectors in its memory in the forward pass and updates the
gradients in the backward pass.</p>
<p>With sharding, there is an added communication cost: each GPU needs to
ask the other GPUs for embedding vector lookup and communicate the
gradients computed as well. This is typically referred to as <code class="docutils literal notranslate"><span class="pre">all2all</span></code>
communication. In TorchRec, for input data on a given GPU, we determine
where the embedding shard for each part of the data is located and send
it to the target GPU. That target GPU then returns the embedding vectors
back to the original GPU. In the backward pass, the gradients are sent
back to the target GPU and the shards are updated accordingly with the
optimizer.</p>
<p>As described above, sharding requires us to communicate the input data
and embedding lookups. TorchRec handles this in three main stages, we
will refer to this as the sharded embedding module forward that is used
in training and inference of a TorchRec model:</p>
<ul class="simple">
<li><p>Feature All to All/Input distribution (<code class="docutils literal notranslate"><span class="pre">input_dist</span></code>)</p>
<ul>
<li><p>Communicate input data (in the form of a <code class="docutils literal notranslate"><span class="pre">KeyedJaggedTensor</span></code>) to
the appropriate device containing relevant embedding table shard</p></li>
</ul>
</li>
<li><p>Embedding Lookup</p>
<ul>
<li><p>Lookup embeddings with new input data formed after feature all to
all exchange</p></li>
</ul>
</li>
<li><p>Embedding All to All/Output Distribution (<code class="docutils literal notranslate"><span class="pre">output_dist</span></code>)</p>
<ul>
<li><p>Communicate embedding lookup data back to the appropriate device
that asked for it (in accordance with the input data the device
received)</p></li>
</ul>
</li>
<li><p>The backward pass does the same operations but in reverse order.</p></li>
</ul>
<p>The diagram below demonstrates how it works:</p>
<figure class="align-center" id="id2">
<img alt="Visualizing the forward pass including the input_dist, lookup, and output_dist of a sharded TorchRec module" src="_images/torchrec_forward.png" />
<figcaption>
<p><span class="caption-text"><em>Figure 2: Forward pass of a table wise sharded table including the input_dist, lookup, and output_dist of a sharded TorchRec module</em></span><a class="headerlink" href="#id2" title="Link to this image">#</a></p>
</figcaption>
</figure>
</section>
<section id="distributedmodelparallel">
<h2>DistributedModelParallel<a class="headerlink" href="#distributedmodelparallel" title="Link to this heading">#</a></h2>
<p>All of the above culminates into the main entrypoint that TorchRec uses
to shard and integrate the plan. At a high level,
<code class="docutils literal notranslate"><span class="pre">DistributedModelParallel</span></code> does the following:</p>
<ul class="simple">
<li><p>Initializes the environment by setting up process groups and
assigning device type.</p></li>
<li><p>Uses default sharders if no sharders are provided, the default includes
<code class="docutils literal notranslate"><span class="pre">EmbeddingBagCollectionSharder</span></code>.</p></li>
<li><p>Takes in the provided sharding plan, if none is provided, it
generates one.</p></li>
<li><p>Creates a sharded version of modules and replaces the original
modules with them, for example, converts <code class="docutils literal notranslate"><span class="pre">EmbeddingCollection</span></code> to
<code class="docutils literal notranslate"><span class="pre">ShardedEmbeddingCollection</span></code>.</p></li>
<li><p>By default, wraps the <code class="docutils literal notranslate"><span class="pre">DistributedModelParallel</span></code> with
<code class="docutils literal notranslate"><span class="pre">DistributedDataParallel</span></code> to make the module both model and data
parallel.</p></li>
</ul>
</section>
<section id="optimizer">
<h2>Optimizer<a class="headerlink" href="#optimizer" title="Link to this heading">#</a></h2>
<p>TorchRec modules provide a seamless API to fuse the backwards pass and
optimizer step in training, providing a significant optimization in
performance and decreasing the memory used, alongside granularity in
assigning distinct optimizers to distinct model parameters.</p>
<figure class="align-center" id="id3">
<img alt="Visualizing fusing of optimizer in backward to update sparse embedding table" src="_images/fused_backward_optimizer.png" />
<figcaption>
<p><span class="caption-text"><em>Figure 3: Fusing embedding backward with sparse optimizer</em></span><a class="headerlink" href="#id3" title="Link to this image">#</a></p>
</figcaption>
</figure>
</section>
<section id="inference">
<h2>Inference<a class="headerlink" href="#inference" title="Link to this heading">#</a></h2>
<p>Inference environments are different from training, they are very
sensitive to performance and the size of the model. There are two key
differences TorchRec inference optimizes for:</p>
<ul class="simple">
<li><p><strong>Quantization:</strong> inference models are quantized for lower latency
and reduced model size. This optimization lets us use as few devices
as possible for inference to minimize latency.</p></li>
<li><p><strong>C++ environment:</strong> to minimize latency even further, the model is
ran in a C++ environment.</p></li>
</ul>
<p>TorchRec provides the following to convert a TorchRec model into being
inference ready:</p>
<ul class="simple">
<li><p>APIs for quantizing the model, including optimizations automatically
with FBGEMM TBE</p></li>
<li><p>Sharding embeddings for distributed inference</p></li>
<li><p>Compiling the model to TorchScript (compatible in C++)</p></li>
</ul>
</section>
<section id="see-also">
<h2>See Also<a class="headerlink" href="#see-also" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/pytorch/torchrec/blob/main/TorchRec_Interactive_Tutorial_Notebook_OSS_version.ipynb">TorchRec Interactive Notebook using the concepts</a></p></li>
</ul>
</section>
</section>


                </article>
              
  </article>
  
              
              
                <footer class="bd-footer-article">
                  <div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item">
<div class="feedback">
  
<div class="rating">
    Rate this Page
    <div class="stars">
        
        <span class="star" data-behavior="tutorial-rating" data-count="1" data-value="1">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="2" data-value="2">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="3" data-value="3">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="4" data-value="4">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="5" data-value="5">★</span>
        
    </div>
</div>

  <div class="feedback-send">
    <button class="feedback-btn"
            onclick="openGitHubIssue()"
            data-bs-title="Create a GitHub Issue"
            data-bs-placement="bottom"
            data-bs-toggle="tooltip"
            data-gtm="feedback-btn-click">Send Feedback
    </button>
  </div>
</div>

<div class="prev-next-area">
    <a class="left-prev"
       href="high-level-arch.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">TorchRec High Level Architecture</p>
      </div>
    </a>
    <a class="right-next"
       href="setup-torchrec.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Setup</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>

<div class="footer-info">
  <p class="copyright">
    
  </p>

  <p class="theme-version">
    Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.15.4.
  </p>
</div>
</div>
  
</div>
                </footer>
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="high-level-arch.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">TorchRec High Level Architecture</p>
      </div>
    </a>
    <a class="right-next"
       href="setup-torchrec.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Setup</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc">
<div class="sidebar-secondary-items sidebar-secondary__inner">
    
       <div class="sidebar-secondary-item">
<div
    id="pst-page-navigation-heading-2"
    class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> On this page
  </div>
  <nav class="bd-toc-nav page-toc" aria-labelledby="pst-page-navigation-heading-2">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#jaggedtensor">JaggedTensor</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#keyedjaggedtensor">KeyedJaggedTensor</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#planner">Planner</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#sharding-of-embeddingtables">Sharding of EmbeddingTables</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#distributed-training-with-torchrec-sharded-modules">Distributed Training with TorchRec Sharded Modules</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#distributedmodelparallel">DistributedModelParallel</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#optimizer">Optimizer</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#inference">Inference</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#see-also">See Also</a></li>
</ul>
  </nav></div>
    
       <div class="sidebar-secondary-item">

  
  <div class="tocsection editthispage">
    <a href="https://github.com/meta-pytorch/torchrec/edit/main/docs/source/concepts.rst">
      <i class="fa-solid fa-pencil"></i>
      
      
        
          Edit on GitHub
        
      
    </a>
  </div>
</div>
    
       <div class="sidebar-secondary-item">
    <div class="tocsection sourcelink">
      <a href="_sources/concepts.rst.txt">
        <i class="fa-solid fa-file-lines"></i> Show Source
      </a>
    </div>
</div>
    




</div>
</div>
              
            
          </div>
          <footer class="bd-footer-content">
            
          </footer>
        
      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  


<style>
.site-footer {
    padding: 20px 40px;
    height: 60px !important;
}

@media screen and (min-width: 768px) {
    .site-footer {
        padding: 20px 40px;
    }
}

.site-footer .privacy-policy {
    border-top: none;
    margin-top: 0px;
}

.site-footer .privacy-policy .copyright {
    padding-top: 0;
}
</style>


<footer class="site-footer">

    <div class="privacy-policy">
      <div class="copyright">
      
        <p>
           Copyright © 2025 Meta Platforms, Inc
        </p>
        
      </div>
    </div>


  </div>
</footer>

<div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="_static/img/pytorch-x.svg">
  </div>
</div>
  
  <footer class="bd-footer">
<div class="bd-footer__inner bd-page-width">
  
    <div class="footer-items__start">
      
        <div class="footer-item">

  <p class="copyright">
    
      © Copyright Meta Platforms, Inc.
      <br/>
    
  </p>
</div>
      
        <div class="footer-item">

  <p class="sphinx-version">
    Created using <a href="https://www.sphinx-doc.org/">Sphinx</a> 7.2.6.
    <br/>
  </p>
</div>
      
    </div>
  
  
  
    <div class="footer-items__end">
      
        <div class="footer-item">
<p class="theme-version">
  Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.15.4.
</p></div>
      
    </div>
  
</div>

  </footer>
  <script type="application/ld+json">
    {
       "@context": "https://schema.org",
       "@type": "Article",
       "name": "TorchRec Concepts",
       "headline": "TorchRec Concepts",
       "description": "PyTorch Documentation. Explore PyTorch, an open-source machine learning library that accelerates the path from research prototyping to production deployment. Discover tutorials, API references, and guides to help you build and deploy deep learning models efficiently.",
       "url": "/concepts.html",
       "articleBody": "TorchRec Concepts# In this section, we will learn about the key concepts of TorchRec, designed to optimize large-scale recommendation systems using PyTorch. We will learn how each concept works in detail and review how it is used with the rest of TorchRec. TorchRec has specific input/output data types of its modules to efficiently represent sparse features, including: JaggedTensor: a wrapper around the lengths/offsets and values tensors for a singular sparse feature. KeyedJaggedTensor: efficiently represent multiple sparse features, can think of it as multiple JaggedTensors. KeyedTensor: a wrapper around torch.Tensor that allows access to tensor values through keys. With the goal of high performance and efficiency, the canonical torch.Tensor is highly inefficient for representing sparse data. TorchRec introduces these new data types because they provide efficient storage and representation of sparse input data. As you will see later on, the KeyedJaggedTensor makes communication of input data in a distributed environment very efficient leading to one of the key performance advantages that TorchRec provides. In the end-to-end training loop, TorchRec comprises of the following main components: Planner: Takes in the configuration of embedding tables, environment setup, and generates an optimized sharding plan for the model. Sharder: Shards model according to sharding plan with different sharding strategies including data-parallel, table-wise, row-wise, table-wise-row-wise, column-wise, and table-wise-column-wise sharding. DistributedModelParallel: Combines sharder, optimizer, and provides an entry point into the training the model in a distributed manner. JaggedTensor# A JaggedTensor represents a sparse feature through lengths, values, and offsets. It is called \u201cjagged\u201d because it efficiently represents data with variable-length sequences. In contrast, a canonical torch.Tensor assumes that each sequence has the same length, which is often not the case with real world data. A JaggedTensor facilitates the representation of such data without padding making it highly efficient. Key Components: Lengths: A list of integers representing the number of elements for each entity. Offsets: A list of integers representing the starting index of each sequence in the flattened values tensor. These provide an alternative to lengths. Values: A 1D tensor containing the actual values for each entity, stored contiguously. Here is a simple example demonstrating how each of the components would look like: # User interactions: # - User 1 interacted with 2 items # - User 2 interacted with 3 items # - User 3 interacted with 1 item lengths = [2, 3, 1] offsets = [0, 2, 5] # Starting index of each user\u0027s interactions values = torch.Tensor([101, 102, 201, 202, 203, 301]) # Item IDs interacted with jt = JaggedTensor(lengths=lengths, values=values) # OR jt = JaggedTensor(offsets=offsets, values=values) KeyedJaggedTensor# A KeyedJaggedTensor extends the functionality of JaggedTensor by introducing keys (which are typically feature names) to label different groups of features, for example, user features and item features. This is the data type used in forward of EmbeddingBagCollection and EmbeddingCollection as they are used to represent multiple features in a table. A KeyedJaggedTensor has an implied batch size, which is the length of lengths tensor divided by the number of keys. The example below has a batch size of 2 (4 lengths divided by 2 keys). Similar to a JaggedTensor, the offsets and lengths function in the same manner. You can also access the lengths, offsets, and values of a feature by accessing the key from the KeyedJaggedTensor. keys = [\"user_features\", \"item_features\"] # Lengths of interactions: # - User features: 2 users, with 2 and 3 interactions respectively # - Item features: 2 items, with 1 and 2 interactions respectively lengths = [2, 3, 1, 2] values = torch.Tensor([11, 12, 21, 22, 23, 101, 201, 202]) # Create a KeyedJaggedTensor kjt = KeyedJaggedTensor(keys=keys, lengths=lengths, values=values) # Access the features by key print(kjt[\"user_features\"]) # Outputs user features print(kjt[\"item_features\"]) Planner# The TorchRec planner helps determine the best sharding configuration for a model. It evaluates multiple possibilities for sharding embedding tables and optimizes for performance. The planner performs the following: Assesses the memory constraints of the hardware. Estimates compute requirements based on memory fetches, such as embedding lookups. Addresses data-specific factors. Considers other hardware specifics, such as bandwidth, to generate an optimal sharding plan. To ensure accurate consideration of these factors, the Planner can incorporate data about the embedding tables, constraints, hardware information, and topology to help in generating an optimal plan. Sharding of EmbeddingTables# TorchRec sharder provides multiple sharding strategies for various use cases, we outline some of the sharding strategies and how they work as well as their benefits and limitations. Generally, we recommend using the TorchRec planner to generate a sharding plan for you as it will find the optimal sharding strategy for each embedding table in your model. Each sharding strategy determines how to do the table split, whether the table should be cut up and how, whether to keep one or a few copies of some tables, and so on. Each piece of the table from the outcome of sharding, whether it is one embedding table or part of it, is referred to as a shard. Figure 1: Visualizing the placement of table shards under different sharding schemes offered in TorchRec# Here is the list of all sharding types available in TorchRec: Table-wise (TW): as the name suggests, embedding table is kept as a whole piece and placed on one rank. Column-wise (CW): the table is split along the emb_dim dimension, for example, emb_dim=256 is split into 4 shards: [64, 64, 64, 64]. Row-wise (RW): the table is split along the hash_size dimension, usually split evenly among all the ranks. Table-wise-row-wise (TWRW): table is placed on one host, split row-wise among the ranks on that host. Grid-shard (GS): a table is CW sharded and each CW shard is placed TWRW on a host. Data parallel (DP): each rank keeps a copy of the table. Once sharded, the modules are converted to sharded versions of themselves, known as ShardedEmbeddingCollection and ShardedEmbeddingBagCollection in TorchRec. These modules handle the communication of input data, embedding lookups, and gradients. Distributed Training with TorchRec Sharded Modules# With many sharding strategies available, how do we determine which one to use? There is a cost associated with each sharding scheme, which in conjunction with model size and number of GPUs determines which sharding strategy is best for a model. Without sharding, where each GPU keeps a copy of the embedding table (DP), the main cost is computation in which each GPU looks up the embedding vectors in its memory in the forward pass and updates the gradients in the backward pass. With sharding, there is an added communication cost: each GPU needs to ask the other GPUs for embedding vector lookup and communicate the gradients computed as well. This is typically referred to as all2all communication. In TorchRec, for input data on a given GPU, we determine where the embedding shard for each part of the data is located and send it to the target GPU. That target GPU then returns the embedding vectors back to the original GPU. In the backward pass, the gradients are sent back to the target GPU and the shards are updated accordingly with the optimizer. As described above, sharding requires us to communicate the input data and embedding lookups. TorchRec handles this in three main stages, we will refer to this as the sharded embedding module forward that is used in training and inference of a TorchRec model: Feature All to All/Input distribution (input_dist) Communicate input data (in the form of a KeyedJaggedTensor) to the appropriate device containing relevant embedding table shard Embedding Lookup Lookup embeddings with new input data formed after feature all to all exchange Embedding All to All/Output Distribution (output_dist) Communicate embedding lookup data back to the appropriate device that asked for it (in accordance with the input data the device received) The backward pass does the same operations but in reverse order. The diagram below demonstrates how it works: Figure 2: Forward pass of a table wise sharded table including the input_dist, lookup, and output_dist of a sharded TorchRec module# DistributedModelParallel# All of the above culminates into the main entrypoint that TorchRec uses to shard and integrate the plan. At a high level, DistributedModelParallel does the following: Initializes the environment by setting up process groups and assigning device type. Uses default sharders if no sharders are provided, the default includes EmbeddingBagCollectionSharder. Takes in the provided sharding plan, if none is provided, it generates one. Creates a sharded version of modules and replaces the original modules with them, for example, converts EmbeddingCollection to ShardedEmbeddingCollection. By default, wraps the DistributedModelParallel with DistributedDataParallel to make the module both model and data parallel. Optimizer# TorchRec modules provide a seamless API to fuse the backwards pass and optimizer step in training, providing a significant optimization in performance and decreasing the memory used, alongside granularity in assigning distinct optimizers to distinct model parameters. Figure 3: Fusing embedding backward with sparse optimizer# Inference# Inference environments are different from training, they are very sensitive to performance and the size of the model. There are two key differences TorchRec inference optimizes for: Quantization: inference models are quantized for lower latency and reduced model size. This optimization lets us use as few devices as possible for inference to minimize latency. C++ environment: to minimize latency even further, the model is ran in a C++ environment. TorchRec provides the following to convert a TorchRec model into being inference ready: APIs for quantizing the model, including optimizations automatically with FBGEMM TBE Sharding embeddings for distributed inference Compiling the model to TorchScript (compatible in C++) See Also# TorchRec Interactive Notebook using the concepts",
       "author": {
         "@type": "Organization",
         "name": "PyTorch Contributors",
         "url": "https://pytorch.org"
       },
       "image": "https://pytorch.org/docs/stable/_static/img/pytorch_seo.png",
       "mainEntityOfPage": {
         "@type": "WebPage",
         "@id": "/concepts.html"
       },
       "datePublished": "2023-01-01T00:00:00Z",
       "dateModified": "2023-01-01T00:00:00Z"
     }
 </script>
  <script>
    // Tutorials Call to action event tracking
    $("[data-behavior='call-to-action-event']").on('click', function () {
      fbq('trackCustom', "Download", {
        tutorialTitle: $('h1:first').text(),
        downloadLink: this.href,
        tutorialLink: window.location.href,
        downloadTitle: $(this).attr("data-response")
      });
      if (typeof gtag === 'function') {
        gtag('event', 'click', {
          'event_category': $(this).attr("data-response"),
          'event_label': $("h1").first().text(),
          'tutorial_link': window.location.href
        });
      }
    });
  </script>
  
  </body>
</html>