
<!DOCTYPE html>


<html lang="en" data-content_root="./" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Planner &#8212; TorchRec 1.0.0 documentation</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=8f2a1f02" />
    <link rel="stylesheet" type="text/css" href="_static/css/theme.css?v=36fba2ff" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="_static/documentation_options.js?v=8d563738"></script>
    <script src="_static/doctools.js?v=888ff710"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="_static/copybutton.js?v=f281be69"></script>
    <script src="_static/design-tabs.js?v=f930bc37"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'planner-api-reference';</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Model Parallel" href="model-parallel-api-reference.html" />
    <link rel="prev" title="Modules" href="modules-api-reference.html" />

  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
<script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/2.3.1/list.min.js"></script>
<script>
  if (window.location.hostname === 'docs.pytorch.org' || window.location.hostname === 'docs-preview.pytorch.org') {
    const script = document.createElement('script');
    script.src = 'https://cmp.osano.com/16A0DbT9yDNIaQkvZ/31b1b91a-e0b6-47ea-bde2-7f2bd13dbe5c/osano.js?variant=one';
    document.head.appendChild(script);
  }
</script>
<script>
  // Cookie banner for non-LF projects
  document.addEventListener('DOMContentLoaded', function () {
    // Hide cookie banner on local environments and LF owned docs
    if (window.location.hostname === 'localhost' ||
      window.location.hostname === '0.0.0.0' ||
      window.location.hostname === '127.0.0.1' ||
      window.location.hostname === 'docs.pytorch.org' ||
      window.location.hostname === 'docs-preview.pytorch.org' ||
      window.location.hostname.startsWith('192.168.')) {
      const banner = document.querySelector('.cookie-banner-wrapper');
      if (banner) {
        banner.style.display = 'none';
      }
    }
  });
</script>
<!-- Conditional CSS for header and footer height adjustment -->

<style>
  :root {
    --header-height: 0px !important;
    --header-height-desktop: 0px !important;
  }
</style>


<style>
  @media (min-width: 1100px) {
    .site-footer {
      height: 300px !important;
    }
  }
</style>

<link rel="stylesheet" type="text/css" href="_static/css/theme.css" crossorigin="anonymous">
<script type="text/javascript" src="_static/js/theme.js"></script>
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Montserrat:wght@300;400&display=swap" rel="stylesheet">
<meta property="og:image" content="https://docs.pytorch.org/docs/stable/_static/img/pytorch_seo.png" />
<link rel="stylesheet" href="_static/webfonts/all.min.css" crossorigin="anonymous">
<meta http-equiv="Content-Security-Policy"
  content="default-src * 'unsafe-inline' 'unsafe-eval' data: blob:; style-src * 'unsafe-inline'; script-src * 'unsafe-inline' 'unsafe-eval' blob:;">
<meta name="pytorch_project" content="">
<!-- Google Tag Manager (noscript) -->
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-NPLPKN5G" height="0" width="0"
    style="display:none;visibility:hidden"></iframe></noscript>
<!-- End Google Tag Manager (noscript) -->
<!-- Google Tag Manager -->
<script>(function (w, d, s, l, i) {
    w[l] = w[l] || []; w[l].push({
      'gtm.start':
        new Date().getTime(), event: 'gtm.js'
    }); var f = d.getElementsByTagName(s)[0],
      j = d.createElement(s), dl = l != 'dataLayer' ? '&l=' + l : ''; j.async = true; j.src =
        'https://www.googletagmanager.com/gtm.js?id=' + i + dl; f.parentNode.insertBefore(j, f);
    j.onload = function () {
      window.dispatchEvent(new Event('gtm_loaded'));
      console.log('GTM loaded successfully');
    };
  })(window, document, 'script', 'dataLayer', 'GTM-NPLPKN5G');
</script>
<!-- End Google Tag Manager -->
<!-- Facebook Pixel Code -->
<script>
  !function (f, b, e, v, n, t, s) {
    if (f.fbq) return; n = f.fbq = function () {
      n.callMethod ?
        n.callMethod.apply(n, arguments) : n.queue.push(arguments)
    };
    if (!f._fbq) f._fbq = n; n.push = n; n.loaded = !0; n.version = '2.0';
    n.queue = []; t = b.createElement(e); t.async = !0;
    t.src = v; s = b.getElementsByTagName(e)[0];
    s.parentNode.insertBefore(t, s)
  }(window, document, 'script',
    'https://connect.facebook.net/en_US/fbevents.js');
  fbq('init', '243028289693773');
  fbq('track', 'PageView');
</script>
<script>
  document.documentElement.setAttribute('data-version', '1.0.0');
</script>
<noscript>
  <img height="1" width="1" src="https://www.facebook.com/tr?id=243028289693773&ev=PageView&noscript=1" />
</noscript>
<script>
  function gtag() {
    window.dataLayer.push(arguments);
  }
</script>
<!-- End Facebook Pixel Code -->
<!-- Repository configuration for tutorials -->

<!-- Script to Fix scrolling -->
<script>
  document.addEventListener('DOMContentLoaded', function () {
    // Fix anchor scrolling
    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
      anchor.addEventListener('click', function (e) {
        e.preventDefault();
        const targetId = this.getAttribute('href').substring(1);
        const targetElement = document.getElementById(targetId);

        if (targetElement) {
          const headerHeight =
            (document.querySelector('.header-holder') ? document.querySelector('.header-holder').offsetHeight : 0) +
            (document.querySelector('.bd-header') ? document.querySelector('.bd-header').offsetHeight : 0) + 20;

          const targetPosition = targetElement.getBoundingClientRect().top + window.pageYOffset - headerHeight;
          window.scrollTo({
            top: targetPosition,
            behavior: 'smooth'
          });

          // Update URL hash without scrolling
          history.pushState(null, null, '#' + targetId);
        }
      });
    });
  });
</script>

<script async src="https://cse.google.com/cse.js?cx=e65585f8c3ea1440e"></script>

  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>

  </head>

<body data-feedback-url="https://github.com/meta-pytorch/torchrec" class="pytorch-body">
  
  
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
<div class="bd-header__inner bd-page-width">
  <button class="pst-navbar-icon sidebar-toggle primary-toggle" aria-label="Site navigation">
    <span class="fa-solid fa-bars"></span>
  </button>
  
  
  <div class=" navbar-header-items__start">
    
      <div class="navbar-item">

  
    
  

<a class="navbar-brand logo" href="index.html">
  
  
  
  
  
  
    <p class="title logo__title">Home</p>
  
</a></div>
    
  </div>
  
  <div class=" navbar-header-items">
    
    <div class="me-auto navbar-header-items__center">
      
        <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item ">
  <a class="nav-link nav-internal" href="intro.html">
    Intro
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="setup-torchrec.html">
    Setup
  </a>
</li>


<li class="nav-item current active">
  <a class="nav-link nav-internal" href="api.html">
    API reference
  </a>
</li>

  </ul>
</nav></div>
      
    </div>
    
    
    <div class="navbar-header-items__end">
      
      
        <div class="navbar-item">
  
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
</div>
      
        <div class="navbar-item">

<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script></div>
      
        <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://x.com/PyTorch" title="X" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-x-twitter fa-lg" aria-hidden="true"></i>
            <span class="sr-only">X</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/meta-pytorch/torchrec" title="GitHub" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">GitHub</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://dev-discuss.pytorch.org/" title="Discourse" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-discourse fa-lg" aria-hidden="true"></i>
            <span class="sr-only">Discourse</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://pypi.org/project/torchrec/" title="PyPi" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-python fa-lg" aria-hidden="true"></i>
            <span class="sr-only">PyPi</span></a>
        </li>
</ul></div>
      
    </div>
    
  </div>
  
  

  
    <button class="pst-navbar-icon sidebar-toggle secondary-toggle" aria-label="On this page">
      <span class="fa-solid fa-outdent"></span>
    </button>
  
</div>

    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
      <div class="sidebar-header-items__center">
        
          
          
            <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item ">
  <a class="nav-link nav-internal" href="intro.html">
    Intro
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="setup-torchrec.html">
    Setup
  </a>
</li>


<li class="nav-item current active">
  <a class="nav-link nav-internal" href="api.html">
    API reference
  </a>
</li>

  </ul>
</nav></div>
          
        
      </div>
    
    
    
      <div class="sidebar-header-items__end">
        
          <div class="navbar-item">
  
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
</div>
        
          <div class="navbar-item">

<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script></div>
        
          <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://x.com/PyTorch" title="X" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-x-twitter fa-lg" aria-hidden="true"></i>
            <span class="sr-only">X</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/meta-pytorch/torchrec" title="GitHub" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">GitHub</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://dev-discuss.pytorch.org/" title="Discourse" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-discourse fa-lg" aria-hidden="true"></i>
            <span class="sr-only">Discourse</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://pypi.org/project/torchrec/" title="PyPi" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-python fa-lg" aria-hidden="true"></i>
            <span class="sr-only">PyPi</span></a>
        </li>
</ul></div>
        
      </div>
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
<nav class="bd-docs-nav bd-links"
     aria-label="Section Navigation">
  <p class="bd-links__title" role="heading" aria-level="1">Section Navigation</p>
  <div class="bd-toc-item navbar-nav"><ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="datatypes-api-reference.html">Data Types</a></li>
<li class="toctree-l1"><a class="reference internal" href="modules-api-reference.html">Modules</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Planner</a></li>
<li class="toctree-l1"><a class="reference internal" href="model-parallel-api-reference.html">Model Parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="inference-api-reference.html">Inference</a></li>
</ul>
</div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        
          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item">



<nav aria-label="Breadcrumb" class="d-print-none">
  <ul class="bd-breadcrumbs">
    
    <li class="breadcrumb-item breadcrumb-home">
      <a href="index.html" class="nav-link" aria-label="Home">
        <i class="fa-solid fa-home"></i>
      </a>
    </li>
    
    <li class="breadcrumb-item"><a href="api.html" class="nav-link">API reference</a></li>
    
    <li class="breadcrumb-item active" aria-current="page">Planner</li>
  </ul>
</nav>
</div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">
<div class="rating">
    Rate this Page
    <div class="stars">
        
        <span class="star" data-behavior="tutorial-rating" data-count="1" data-value="1">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="2" data-value="2">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="3" data-value="3">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="4" data-value="4">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="5" data-value="5">★</span>
        
    </div>
</div>
</div>
      
    </div>
  
</div>
</div>
              
              
  
<div id="searchbox"></div>
  <article class="bd-article" id="pytorch-article">
    <!-- Hidden breadcrumb schema for SEO only -->
    <div style="display:none;" itemscope itemtype="https://schema.org/BreadcrumbList">
      
      <div itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
        <link itemprop="item" href="api.html">
        <meta itemprop="name" content="API reference">
        <meta itemprop="position" content="1">
      </div>
      
      <div itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
        <meta itemprop="name" content="Planner">
        <meta itemprop="position" content="2">
      </div>
    </div>

    
    

    
              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section id="planner">
<h1>Planner<a class="headerlink" href="#planner" title="Link to this heading">#</a></h1>
<p>The TorchRec Planner is responsible for determining the most performant, balanced
sharding plan for distributed training and inference.</p>
<p>The main API for generating a sharding plan is <code class="docutils literal notranslate"><span class="pre">EmbeddingShardingPlanner.plan</span></code></p>
<dl class="py class" id="module-torchrec.distributed.types">
<dt class="sig sig-object py" id="torchrec.distributed.types.ShardingPlan">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torchrec.distributed.types.</span></span><span class="sig-name descname"><span class="pre">ShardingPlan</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">plan</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">ModuleShardingPlan</span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#torchrec.distributed.types.ShardingPlan" title="Link to this definition">#</a></dt>
<dd><p>Representation of sharding plan. This uses the FQN of the larger wrapped model (i.e the model that is wrapped using <cite>DistributedModelParallel</cite>)
EmbeddingModuleShardingPlan should be used when TorchRec composability is desired.</p>
<dl class="py attribute">
<dt class="sig sig-object py" id="torchrec.distributed.types.ShardingPlan.plan">
<span class="sig-name descname"><span class="pre">plan</span></span><a class="headerlink" href="#torchrec.distributed.types.ShardingPlan.plan" title="Link to this definition">#</a></dt>
<dd><p>dict keyed by module path of
dict of parameter sharding specs keyed by parameter name.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>Dict[str, EmbeddingModuleShardingPlan]</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torchrec.distributed.types.ShardingPlan.get_plan_for_module">
<span class="sig-name descname"><span class="pre">get_plan_for_module</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">module_path</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">ModuleShardingPlan</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span></span><a class="headerlink" href="#torchrec.distributed.types.ShardingPlan.get_plan_for_module" title="Link to this definition">#</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>module_path</strong> (<em>str</em>) – </p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>dict of parameter sharding specs keyed by parameter name. None if sharding specs do not exist for given module_path.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>Optional[ModuleShardingPlan]</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class" id="module-torchrec.distributed.planner.planners">
<dt class="sig sig-object py" id="torchrec.distributed.planner.planners.EmbeddingShardingPlanner">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torchrec.distributed.planner.planners.</span></span><span class="sig-name descname"><span class="pre">EmbeddingShardingPlanner</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">topology</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Topology</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">enumerator</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Enumerator</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">storage_reservation</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">StorageReservation</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">proposer</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Proposer</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">Proposer</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">partitioner</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Partitioner</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">performance_model</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">PerfModel</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stats</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Stats</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">Stats</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">constraints</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">ParameterConstraints</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">debug</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">callbacks</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">Callable</span><span class="p"><span class="pre">[</span></span><span class="p"><span class="pre">[</span></span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">ShardingOption</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">ShardingOption</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">timeout_seconds</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">plan_loader</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">PlanLoader</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#torchrec.distributed.planner.planners.EmbeddingShardingPlanner" title="Link to this definition">#</a></dt>
<dd><p>Provides an optimized sharding plan for a given module with shardable parameters
according to the provided sharders, topology, and constraints.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>topology</strong> (<em>Optional</em><em>[</em><em>Topology</em><em>]</em>) – the topology of the current process group.</p></li>
<li><p><strong>batch_size</strong> (<em>Optional</em><em>[</em><em>int</em><em>]</em>) – the batch size of the model.</p></li>
<li><p><strong>enumerator</strong> (<em>Optional</em><em>[</em><em>Enumerator</em><em>]</em>) – the enumerator to use</p></li>
<li><p><strong>storage_reservation</strong> (<em>Optional</em><em>[</em><em>StorageReservation</em><em>]</em>) – the storage reservation to use</p></li>
<li><p><strong>proposer</strong> (<em>Optional</em><em>[</em><em>Union</em><em>[</em><em>Proposer</em><em>, </em><em>List</em><em>[</em><em>Proposer</em><em>]</em><em>]</em><em>]</em>) – the proposer(s) to use</p></li>
<li><p><strong>partitioner</strong> (<em>Optional</em><em>[</em><em>Partitioner</em><em>]</em>) – the partitioner to use</p></li>
<li><p><strong>performance_model</strong> (<em>Optional</em><em>[</em><em>PerfModel</em><em>]</em>) – the performance model to use</p></li>
<li><p><strong>stats</strong> (<em>Optional</em><em>[</em><em>Union</em><em>[</em><em>Stats</em><em>, </em><em>List</em><em>[</em><em>Stats</em><em>]</em><em>]</em><em>]</em>) – the stats to use</p></li>
<li><p><strong>constraints</strong> (<em>Optional</em><em>[</em><em>Dict</em><em>[</em><em>str</em><em>, </em><em>ParameterConstraints</em><em>]</em><em>]</em>) – per table constraints
for sharding.</p></li>
<li><p><strong>debug</strong> (<em>bool</em>) – whether to print debug information.</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">ebc</span> <span class="o">=</span> <span class="n">EmbeddingBagCollection</span><span class="p">(</span><span class="n">tables</span><span class="o">=</span><span class="n">eb_configs</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;meta&quot;</span><span class="p">))</span>
<span class="n">planner</span> <span class="o">=</span> <span class="n">EmbeddingShardingPlanner</span><span class="p">()</span>
<span class="n">plan</span> <span class="o">=</span> <span class="n">planner</span><span class="o">.</span><span class="n">plan</span><span class="p">(</span>
    <span class="n">module</span><span class="o">=</span><span class="n">ebc</span><span class="p">,</span>
    <span class="n">sharders</span><span class="o">=</span><span class="p">[</span><span class="n">EmbeddingBagCollectionSharder</span><span class="p">()],</span>
<span class="p">)</span>
</pre></div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="torchrec.distributed.planner.planners.EmbeddingShardingPlanner.collective_plan">
<span class="sig-name descname"><span class="pre">collective_plan</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">module</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Module</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sharders</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">ModuleSharder</span><span class="p"><span class="pre">[</span></span><span class="pre">Module</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pg</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">ProcessGroup</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#torchrec.distributed.types.ShardingPlan" title="torchrec.distributed.types.ShardingPlan"><span class="pre">ShardingPlan</span></a></span></span><a class="headerlink" href="#torchrec.distributed.planner.planners.EmbeddingShardingPlanner.collective_plan" title="Link to this definition">#</a></dt>
<dd><p>Call self.plan(…) on rank 0 and broadcast</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>module</strong> (<em>nn.Module</em>) – the module to shard.</p></li>
<li><p><strong>sharders</strong> (<em>Optional</em><em>[</em><em>List</em><em>[</em><em>ModuleSharder</em><em>[</em><em>nn.Module</em><em>]</em><em>]</em><em>]</em>) – the sharders to use for sharding</p></li>
<li><p><strong>pg</strong> (<em>Optional</em><em>[</em><em>dist.ProcessGroup</em><em>]</em>) – the process group to use for collective operations</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>the sharding plan for the module.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="#torchrec.distributed.types.ShardingPlan" title="torchrec.distributed.types.ShardingPlan">ShardingPlan</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torchrec.distributed.planner.planners.EmbeddingShardingPlanner.plan">
<span class="sig-name descname"><span class="pre">plan</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">module</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Module</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sharders</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">ModuleSharder</span><span class="p"><span class="pre">[</span></span><span class="pre">Module</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#torchrec.distributed.types.ShardingPlan" title="torchrec.distributed.types.ShardingPlan"><span class="pre">ShardingPlan</span></a></span></span><a class="headerlink" href="#torchrec.distributed.planner.planners.EmbeddingShardingPlanner.plan" title="Link to this definition">#</a></dt>
<dd><p>Provides an optimized sharding plan for a given module with shardable parameters
according to the provided sharders, topology, and constraints.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>module</strong> (<em>nn.Module</em>) – the module to shard.</p></li>
<li><p><strong>sharders</strong> (<em>List</em><em>[</em><em>ModuleSharder</em><em>[</em><em>nn.Module</em><em>]</em><em>]</em>) – the sharders to use for sharding.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>the sharding plan for the module.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="#torchrec.distributed.types.ShardingPlan" title="torchrec.distributed.types.ShardingPlan">ShardingPlan</a></p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class" id="module-torchrec.distributed.planner.enumerators">
<dt class="sig sig-object py" id="torchrec.distributed.planner.enumerators.EmbeddingEnumerator">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torchrec.distributed.planner.enumerators.</span></span><span class="sig-name descname"><span class="pre">EmbeddingEnumerator</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">topology</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Topology</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">constraints</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">ParameterConstraints</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">estimator</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">ShardEstimator</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">ShardEstimator</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_exact_enumerate_order</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#torchrec.distributed.planner.enumerators.EmbeddingEnumerator" title="Link to this definition">#</a></dt>
<dd><p>Generates embedding sharding options for given <cite>nn.Module</cite>, considering user provided
constraints.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>topology</strong> (<em>Topology</em>) – device topology.</p></li>
<li><p><strong>batch_size</strong> (<em>int</em>) – batch size.</p></li>
<li><p><strong>constraints</strong> (<em>Optional</em><em>[</em><em>Dict</em><em>[</em><em>str</em><em>, </em><em>ParameterConstraints</em><em>]</em><em>]</em>) – dict of parameter names
to provided ParameterConstraints.</p></li>
<li><p><strong>estimator</strong> (<em>Optional</em><em>[</em><em>Union</em><em>[</em><em>ShardEstimator</em><em>, </em><em>List</em><em>[</em><em>ShardEstimator</em><em>]</em><em>]</em><em>]</em>) – shard performance estimators.</p></li>
<li><p><strong>use_exact_enumerate_order</strong> (<em>bool</em>) – whether to enumerate shardable parameters in the exact name_children enumeration order</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="torchrec.distributed.planner.enumerators.EmbeddingEnumerator.enumerate">
<span class="sig-name descname"><span class="pre">enumerate</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">module</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Module</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sharders</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">ModuleSharder</span><span class="p"><span class="pre">[</span></span><span class="pre">Module</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">ShardingOption</span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#torchrec.distributed.planner.enumerators.EmbeddingEnumerator.enumerate" title="Link to this definition">#</a></dt>
<dd><p>Generates relevant sharding options given module and sharders.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>module</strong> (<em>nn.Module</em>) – module to be sharded.</p></li>
<li><p><strong>sharders</strong> (<em>List</em><em>[</em><em>ModuleSharder</em><em>[</em><em>nn.Module</em><em>]</em><em>]</em>) – provided sharders for module.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>valid sharding options with values populated.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>List[ShardingOption]</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torchrec.distributed.planner.enumerators.EmbeddingEnumerator.populate_estimates">
<span class="sig-name descname"><span class="pre">populate_estimates</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">sharding_options</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">ShardingOption</span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="headerlink" href="#torchrec.distributed.planner.enumerators.EmbeddingEnumerator.populate_estimates" title="Link to this definition">#</a></dt>
<dd><p>See class description.</p>
</dd></dl>

</dd></dl>

<dl class="py class" id="module-torchrec.distributed.planner.partitioners">
<dt class="sig sig-object py" id="torchrec.distributed.planner.partitioners.GreedyPerfPartitioner">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torchrec.distributed.planner.partitioners.</span></span><span class="sig-name descname"><span class="pre">GreedyPerfPartitioner</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">sort_by</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">SortBy</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">SortBy.STORAGE</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">balance_modules</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#torchrec.distributed.planner.partitioners.GreedyPerfPartitioner" title="Link to this definition">#</a></dt>
<dd><p>Greedy Partitioner.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>sort_by</strong> (<em>SortBy</em>) – Sort sharding options by storage or perf in
descending order (i.e., large tables will be placed first).</p></li>
<li><p><strong>balance_modules</strong> (<em>bool</em>) – Whether to sort by modules first, where
smaller modules will be sorted first. In effect, this will place
tables in each module in a balanced way.</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="torchrec.distributed.planner.partitioners.GreedyPerfPartitioner.partition">
<span class="sig-name descname"><span class="pre">partition</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">proposal</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">ShardingOption</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">storage_constraint</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Topology</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">hbm_per_device</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">ShardingOption</span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#torchrec.distributed.planner.partitioners.GreedyPerfPartitioner.partition" title="Link to this definition">#</a></dt>
<dd><p>Places sharding options on topology based on each sharding option’s
<cite>partition_by</cite> attribute.
The topology, storage, and perfs are updated at the end of the placement.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>proposal</strong> (<em>List</em><em>[</em><em>ShardingOption</em><em>]</em>) – list of populated sharding options.</p></li>
<li><p><strong>storage_constraint</strong> (<em>Topology</em>) – device topology.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>list of sharding options for selected plan.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>List[ShardingOption]</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">sharding_options</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">ShardingOption</span><span class="p">(</span><span class="n">partition_by</span><span class="o">=</span><span class="s2">&quot;uniform&quot;</span><span class="p">,</span>
                <span class="n">shards</span><span class="o">=</span><span class="p">[</span>
                    <span class="n">Shards</span><span class="p">(</span><span class="n">storage</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">perf</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
                    <span class="n">Shards</span><span class="p">(</span><span class="n">storage</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">perf</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
                <span class="p">]),</span>
        <span class="n">ShardingOption</span><span class="p">(</span><span class="n">partition_by</span><span class="o">=</span><span class="s2">&quot;uniform&quot;</span><span class="p">,</span>
                <span class="n">shards</span><span class="o">=</span><span class="p">[</span>
                    <span class="n">Shards</span><span class="p">(</span><span class="n">storage</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">perf</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span>
                    <span class="n">Shards</span><span class="p">(</span><span class="n">storage</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">perf</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span>
                <span class="p">]),</span>
        <span class="n">ShardingOption</span><span class="p">(</span><span class="n">partition_by</span><span class="o">=</span><span class="s2">&quot;device&quot;</span><span class="p">,</span>
                <span class="n">shards</span><span class="o">=</span><span class="p">[</span>
                    <span class="n">Shards</span><span class="p">(</span><span class="n">storage</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">perf</span><span class="o">=</span><span class="mi">3</span><span class="p">),</span>
                    <span class="n">Shards</span><span class="p">(</span><span class="n">storage</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">perf</span><span class="o">=</span><span class="mi">3</span><span class="p">),</span>
                <span class="p">])</span>
        <span class="n">ShardingOption</span><span class="p">(</span><span class="n">partition_by</span><span class="o">=</span><span class="s2">&quot;device&quot;</span><span class="p">,</span>
                <span class="n">shards</span><span class="o">=</span><span class="p">[</span>
                    <span class="n">Shards</span><span class="p">(</span><span class="n">storage</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">perf</span><span class="o">=</span><span class="mi">4</span><span class="p">),</span>
                    <span class="n">Shards</span><span class="p">(</span><span class="n">storage</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">perf</span><span class="o">=</span><span class="mi">4</span><span class="p">),</span>
                <span class="p">]),</span>
    <span class="p">]</span>
<span class="n">topology</span> <span class="o">=</span> <span class="n">Topology</span><span class="p">(</span><span class="n">world_size</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="c1"># First [sharding_options[0] and sharding_options[1]] will be placed on the</span>
<span class="c1"># topology with the uniform strategy, resulting in</span>

<span class="n">topology</span><span class="o">.</span><span class="n">devices</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">perf</span><span class="o">.</span><span class="n">total</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>
<span class="n">topology</span><span class="o">.</span><span class="n">devices</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">perf</span><span class="o">.</span><span class="n">total</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>

<span class="c1"># Finally sharding_options[2] and sharding_options[3]] will be placed on the</span>
<span class="c1"># topology with the device strategy (see docstring of `partition_by_device` for</span>
<span class="c1"># more details).</span>

<span class="n">topology</span><span class="o">.</span><span class="n">devices</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">perf</span><span class="o">.</span><span class="n">total</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">)</span>
<span class="n">topology</span><span class="o">.</span><span class="n">devices</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">perf</span><span class="o">.</span><span class="n">total</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">)</span>

<span class="c1"># The topology updates are done after the end of all the placements (the other</span>
<span class="c1"># in the example is just for clarity).</span>
</pre></div>
</div>
</dd></dl>

</dd></dl>

<dl class="py class" id="module-torchrec.distributed.planner.storage_reservations">
<dt class="sig sig-object py" id="torchrec.distributed.planner.storage_reservations.HeuristicalStorageReservation">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torchrec.distributed.planner.storage_reservations.</span></span><span class="sig-name descname"><span class="pre">HeuristicalStorageReservation</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">percentage</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">parameter_multiplier</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">6.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dense_tensor_estimate</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#torchrec.distributed.planner.storage_reservations.HeuristicalStorageReservation" title="Link to this definition">#</a></dt>
<dd><p>Reserves storage for model to be sharded with heuristical calculation. The storage
reservation is comprised of dense tensor storage, KJT storage, and an extra
percentage of total storage.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>percentage</strong> (<em>float</em>) – extra storage percent to reserve that acts as a margin of
error beyond heuristic calculation of storage.</p></li>
<li><p><strong>parameter_multiplier</strong> (<em>float</em>) – heuristic multiplier for total parameter storage.</p></li>
<li><p><strong>dense_tensor_estimate</strong> (<em>Optional</em><em>[</em><em>int</em><em>]</em>) – storage estimate for dense tensors, uses
default heuristic estimate if not provided.</p></li>
</ul>
</dd>
</dl>
<dl class="py property">
<dt class="sig sig-object py" id="torchrec.distributed.planner.storage_reservations.HeuristicalStorageReservation.last_reserved_topology">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">last_reserved_topology</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Topology</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></em><a class="headerlink" href="#torchrec.distributed.planner.storage_reservations.HeuristicalStorageReservation.last_reserved_topology" title="Link to this definition">#</a></dt>
<dd><p>Cached value of the most recent output from the reserve() method.</p>
</dd></dl>

</dd></dl>

<dl class="py class" id="module-torchrec.distributed.planner.proposers">
<dt class="sig sig-object py" id="torchrec.distributed.planner.proposers.GreedyProposer">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torchrec.distributed.planner.proposers.</span></span><span class="sig-name descname"><span class="pre">GreedyProposer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">use_depth</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">threshold</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#torchrec.distributed.planner.proposers.GreedyProposer" title="Link to this definition">#</a></dt>
<dd><p>Proposes sharding plans in greedy fashion.</p>
<p>Sorts sharding options for each shardable parameter by perf.
On each iteration, finds parameter with largest current storage usage and tries its
next sharding option.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>use_depth</strong> (<em>bool</em>) – When enabled, sharding_options of a fqn are sorted based on
<cite>max(shard.perf.total)</cite>, otherwise sharding_options are sorted by
<cite>sum(shard.perf.total)</cite>.</p></li>
<li><p><strong>threshold</strong> (<em>Optional</em><em>[</em><em>int</em><em>]</em>) – Threshold for early stopping. When specified, the
proposer stops proposing when the proposals have consecutive worse perf_rating
than best_perf_rating.</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="torchrec.distributed.planner.proposers.GreedyProposer.feedback">
<span class="sig-name descname"><span class="pre">feedback</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">partitionable</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">plan</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">ShardingOption</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">perf_rating</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">storage_constraint</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Topology</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="headerlink" href="#torchrec.distributed.planner.proposers.GreedyProposer.feedback" title="Link to this definition">#</a></dt>
<dd><p>Provide feedback to proposer.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>partitionable</strong> (<em>bool</em>) – whether the plan is partitionable.</p></li>
<li><p><strong>plan</strong> (<em>Optional</em><em>[</em><em>List</em><em>[</em><em>ShardingOption</em><em>]</em><em>]</em>) – plan to provide feedback on.</p></li>
<li><p><strong>perf_rating</strong> (<em>Optional</em><em>[</em><em>float</em><em>]</em>) – performance rating of the plan.</p></li>
<li><p><strong>storage_constraint</strong> (<em>Optional</em><em>[</em><em>Topology</em><em>]</em>) – storage constraint of the plan.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torchrec.distributed.planner.proposers.GreedyProposer.load">
<span class="sig-name descname"><span class="pre">load</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">search_space</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">ShardingOption</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">enumerator</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Enumerator</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="headerlink" href="#torchrec.distributed.planner.proposers.GreedyProposer.load" title="Link to this definition">#</a></dt>
<dd><p>Load search space into proposer.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>search_space</strong> (<em>List</em><em>[</em><em>ShardingOption</em><em>]</em>) – search space to load.</p></li>
<li><p><strong>enumerator</strong> (<em>Enumerator</em>) – enumerator used to generate search space.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torchrec.distributed.planner.proposers.GreedyProposer.propose">
<span class="sig-name descname"><span class="pre">propose</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">ShardingOption</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span></span><a class="headerlink" href="#torchrec.distributed.planner.proposers.GreedyProposer.propose" title="Link to this definition">#</a></dt>
<dd><p>Propose a sharding plan.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>proposed plan.</p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p>Optional[List[ShardingOption]]</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class" id="module-torchrec.distributed.planner.shard_estimators">
<dt class="sig sig-object py" id="torchrec.distributed.planner.shard_estimators.EmbeddingPerfEstimator">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torchrec.distributed.planner.shard_estimators.</span></span><span class="sig-name descname"><span class="pre">EmbeddingPerfEstimator</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">topology</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Topology</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">constraints</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">ParameterConstraints</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">is_inference</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#torchrec.distributed.planner.shard_estimators.EmbeddingPerfEstimator" title="Link to this definition">#</a></dt>
<dd><p>Embedding Wall Time Perf Estimator. This estimator estimates the wall time
of a given sharding option.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>topology</strong> (<em>Topology</em>) – device topology.</p></li>
<li><p><strong>constraints</strong> (<em>Optional</em><em>[</em><em>Dict</em><em>[</em><em>str</em><em>, </em><em>ParameterConstraints</em><em>]</em><em>]</em>) – parameter constraints.</p></li>
<li><p><strong>is_inference</strong> (<em>bool</em>) – whether or not the estimator is used for inference.</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="torchrec.distributed.planner.shard_estimators.EmbeddingPerfEstimator.estimate">
<span class="sig-name descname"><span class="pre">estimate</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">sharding_options</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">ShardingOption</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sharder_map</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">ModuleSharder</span><span class="p"><span class="pre">[</span></span><span class="pre">Module</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="headerlink" href="#torchrec.distributed.planner.shard_estimators.EmbeddingPerfEstimator.estimate" title="Link to this definition">#</a></dt>
<dd><p>Estimates the wall time of a given sharding option.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>sharding_options</strong> (<em>List</em><em>[</em><em>ShardingOption</em><em>]</em>) – list of sharding options.</p></li>
<li><p><strong>sharder_map</strong> (<em>Optional</em><em>[</em><em>Dict</em><em>[</em><em>str</em><em>, </em><em>ModuleSharder</em><em>[</em><em>nn.Module</em><em>]</em><em>]</em><em>]</em>) – sharder map.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torchrec.distributed.planner.shard_estimators.EmbeddingPerfEstimator.perf_func_emb_wall_time">
<em class="property"><span class="pre">classmethod</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">perf_func_emb_wall_time</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">shard_sizes</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">compute_kernel</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">compute_device</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sharding_type</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_sizes</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">world_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">local_world_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input_lengths</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">float</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input_data_type_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">table_data_type_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_data_type_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fwd_a2a_comm_data_type_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bwd_a2a_comm_data_type_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fwd_sr_comm_data_type_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bwd_sr_comm_data_type_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_poolings</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">float</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">hbm_mem_bw</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ddr_mem_bw</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">hbm_to_ddr_mem_bw</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">comms_bandwidths</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">GeneralizedCommsBandwidth</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bwd_compute_multiplier</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weighted_feature_bwd_compute_multiplier</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">is_pooled</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">is_weighted</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">caching_ratio</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">is_inference</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prefetch_pipeline</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">expected_cache_fetches</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">uneven_sharding_perf_multiplier</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1.0</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">Perf</span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#torchrec.distributed.planner.shard_estimators.EmbeddingPerfEstimator.perf_func_emb_wall_time" title="Link to this definition">#</a></dt>
<dd><p>Attempts to model perfs as a function of relative wall times.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>shard_sizes</strong> (<em>List</em><em>[</em><em>List</em><em>[</em><em>int</em><em>]</em><em>]</em>) – the list of (local_rows, local_cols) of each
shard.</p></li>
<li><p><strong>compute_kernel</strong> (<em>str</em>) – compute kernel.</p></li>
<li><p><strong>compute_device</strong> (<em>str</em>) – compute device.</p></li>
<li><p><strong>sharding_type</strong> (<em>str</em>) – tw, rw, cw, twrw, dp.</p></li>
<li><p><strong>batch_sizes</strong> (<em>List</em><em>[</em><em>int</em><em>]</em>) – batch size for each input feature.</p></li>
<li><p><strong>world_size</strong> (<em>int</em>) – the number of devices for all hosts.</p></li>
<li><p><strong>local_world_size</strong> (<em>int</em>) – the number of the device for each host.</p></li>
<li><p><strong>input_lengths</strong> (<em>List</em><em>[</em><em>float</em><em>]</em>) – the list of the average number of lookups of each
input query feature. Also referred to as pooling_mean, and it’s equal to
the pooling_factor * num_pooling.</p></li>
<li><p><strong>input_data_type_size</strong> (<em>float</em>) – the data type size of the distributed
data_parallel input.</p></li>
<li><p><strong>table_data_type_size</strong> (<em>float</em>) – the data type size of the table.</p></li>
<li><p><strong>output_data_type_size</strong> (<em>float</em>) – the data type size of the output embeddings.</p></li>
<li><p><strong>fwd_comm_data_type_size</strong> (<em>float</em>) – the data type size of the distributed
data_parallel input during forward communication.</p></li>
<li><p><strong>bwd_comm_data_type_size</strong> (<em>float</em>) – the data type size of the distributed
data_parallel input during backward communication.</p></li>
<li><p><strong>num_poolings</strong> (<em>List</em><em>[</em><em>float</em><em>]</em>) – number of poolings per sample, typically 1.0 for
non-EBF use cases. In EBF use cases, this is the number of events per sample.</p></li>
<li><p><strong>hbm_mem_bw</strong> (<em>float</em>) – the bandwidth of the device HBM.</p></li>
<li><p><strong>ddr_mem_bw</strong> (<em>float</em>) – the bandwidth of the system DDR memory.</p></li>
<li><p><strong>hbm_to_ddr_bw</strong> (<em>float</em>) – the bandwidth between device HBM and system DDR.</p></li>
<li><p><strong>intra_host_bw</strong> (<em>float</em>) – the bandwidth within a single host like multiple threads.</p></li>
<li><p><strong>inter_host_bw</strong> (<em>float</em>) – the bandwidth between two hosts like multiple machines.</p></li>
<li><p><strong>is_pooled</strong> (<em>bool</em>) – True if embedding output is pooled (ie. <cite>EmbeddingBag</cite>), False
if unpooled/sequential (ie. <cite>Embedding</cite>).</p></li>
<li><p><strong>is_weighted</strong> (<em>bool = False</em>) – if the module is an EBC and is weighted, typically
signifying an id score list feature.</p></li>
<li><p><strong>is_inference</strong> (<em>bool = False</em>) – if planning for inference.</p></li>
<li><p><strong>caching_ratio</strong> (<em>Optional</em><em>[</em><em>float</em><em>] </em><em>= None</em>) – cache ratio to determine the bandwidth
of device.</p></li>
<li><p><strong>prefetch_pipeline</strong> (<em>bool = False</em>) – whether prefetch pipeline is enabled.</p></li>
<li><p><strong>expected_cache_fetches</strong> (<em>float</em>) – number of expected cache fetches across global batch</p></li>
<li><p><strong>uneven_sharding_perf_multiplier</strong> (<em>float = 1.0</em>) – multiplier to account for uneven sharding perf</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>the list of perf for each shard.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>List[float]</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class" id="module-0">
<dt class="sig sig-object py" id="torchrec.distributed.planner.shard_estimators.EmbeddingStorageEstimator">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torchrec.distributed.planner.shard_estimators.</span></span><span class="sig-name descname"><span class="pre">EmbeddingStorageEstimator</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">topology</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Topology</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">constraints</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">ParameterConstraints</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pipeline_type</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">PipelineType</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">PipelineType.NONE</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">run_embedding_at_peak_memory</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">is_inference</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#torchrec.distributed.planner.shard_estimators.EmbeddingStorageEstimator" title="Link to this definition">#</a></dt>
<dd><p>Embedding Storage Usage Estimator</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>topology</strong> (<em>Topology</em>) – device topology.</p></li>
<li><p><strong>constraints</strong> (<em>Optional</em><em>[</em><em>Dict</em><em>[</em><em>str</em><em>, </em><em>ParameterConstraints</em><em>]</em><em>]</em>) – parameter constraints.</p></li>
<li><p><strong>pipeline_type</strong> (<em>PipelineType</em>) – The type of pipeline, if any. Will determine the
input replication factor during memory estimation.</p></li>
<li><p><strong>run_embedding_at_peak_memory</strong> (<em>bool</em>) – <p>If the embedding fwd/bwd will be execute when HBM
usage is at peak. When set to TRUE, any temporary memory allocation during
embedding forward/backward, as long as output sizes before output_dist will
be counted towards HBM storage cost. Otherwise they won’t since they’ll be
“hidden” by the real memory peak.</p>
<p>Only take effect if pipeline_type is set for backward compatibility (not affecting
models using old pipeline-agnostic formula)</p>
<p>Default to false because this is typically false for RecSys since memory
peak happens at the end of dense forwrad / beginning of dense backward instead.</p>
</p></li>
<li><p><strong>is_inference</strong> (<em>bool</em>) – If the model is inference model. Default to False.</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="torchrec.distributed.planner.shard_estimators.EmbeddingStorageEstimator.estimate">
<span class="sig-name descname"><span class="pre">estimate</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">sharding_options</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">ShardingOption</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sharder_map</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">ModuleSharder</span><span class="p"><span class="pre">[</span></span><span class="pre">Module</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="headerlink" href="#torchrec.distributed.planner.shard_estimators.EmbeddingStorageEstimator.estimate" title="Link to this definition">#</a></dt>
<dd><p>Estimate the storage cost of each sharding option.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>sharding_options</strong> (<em>List</em><em>[</em><em>ShardingOption</em><em>]</em>) – list of sharding options.</p></li>
<li><p><strong>sharder_map</strong> (<em>Optional</em><em>[</em><em>Dict</em><em>[</em><em>str</em><em>, </em><em>ModuleSharder</em><em>[</em><em>nn.Module</em><em>]</em><em>]</em><em>]</em>) – map from module
type to sharder.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>


                </article>
              
  </article>
  
              
              
                <footer class="bd-footer-article">
                  <div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item">
<div class="feedback">
  
<div class="rating">
    Rate this Page
    <div class="stars">
        
        <span class="star" data-behavior="tutorial-rating" data-count="1" data-value="1">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="2" data-value="2">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="3" data-value="3">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="4" data-value="4">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="5" data-value="5">★</span>
        
    </div>
</div>

  <div class="feedback-send">
    <button class="feedback-btn"
            onclick="openGitHubIssue()"
            data-bs-title="Create a GitHub Issue"
            data-bs-placement="bottom"
            data-bs-toggle="tooltip"
            data-gtm="feedback-btn-click">Send Feedback
    </button>
  </div>
</div>

<div class="prev-next-area">
    <a class="left-prev"
       href="modules-api-reference.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Modules</p>
      </div>
    </a>
    <a class="right-next"
       href="model-parallel-api-reference.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Model Parallel</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>

<div class="footer-info">
  <p class="copyright">
    
  </p>

  <p class="theme-version">
    Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.15.4.
  </p>
</div>
</div>
  
</div>
                </footer>
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="modules-api-reference.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Modules</p>
      </div>
    </a>
    <a class="right-next"
       href="model-parallel-api-reference.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Model Parallel</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc">
<div class="sidebar-secondary-items sidebar-secondary__inner">
    
       <div class="sidebar-secondary-item">
<div
    id="pst-page-navigation-heading-2"
    class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> On this page
  </div>
  <nav class="bd-toc-nav page-toc" aria-labelledby="pst-page-navigation-heading-2">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#torchrec.distributed.types.ShardingPlan"><code class="docutils literal notranslate"><span class="pre">ShardingPlan</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torchrec.distributed.types.ShardingPlan.plan"><code class="docutils literal notranslate"><span class="pre">ShardingPlan.plan</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torchrec.distributed.types.ShardingPlan.get_plan_for_module"><code class="docutils literal notranslate"><span class="pre">ShardingPlan.get_plan_for_module()</span></code></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#torchrec.distributed.planner.planners.EmbeddingShardingPlanner"><code class="docutils literal notranslate"><span class="pre">EmbeddingShardingPlanner</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torchrec.distributed.planner.planners.EmbeddingShardingPlanner.collective_plan"><code class="docutils literal notranslate"><span class="pre">EmbeddingShardingPlanner.collective_plan()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torchrec.distributed.planner.planners.EmbeddingShardingPlanner.plan"><code class="docutils literal notranslate"><span class="pre">EmbeddingShardingPlanner.plan()</span></code></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#torchrec.distributed.planner.enumerators.EmbeddingEnumerator"><code class="docutils literal notranslate"><span class="pre">EmbeddingEnumerator</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torchrec.distributed.planner.enumerators.EmbeddingEnumerator.enumerate"><code class="docutils literal notranslate"><span class="pre">EmbeddingEnumerator.enumerate()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torchrec.distributed.planner.enumerators.EmbeddingEnumerator.populate_estimates"><code class="docutils literal notranslate"><span class="pre">EmbeddingEnumerator.populate_estimates()</span></code></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#torchrec.distributed.planner.partitioners.GreedyPerfPartitioner"><code class="docutils literal notranslate"><span class="pre">GreedyPerfPartitioner</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torchrec.distributed.planner.partitioners.GreedyPerfPartitioner.partition"><code class="docutils literal notranslate"><span class="pre">GreedyPerfPartitioner.partition()</span></code></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#torchrec.distributed.planner.storage_reservations.HeuristicalStorageReservation"><code class="docutils literal notranslate"><span class="pre">HeuristicalStorageReservation</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torchrec.distributed.planner.storage_reservations.HeuristicalStorageReservation.last_reserved_topology"><code class="docutils literal notranslate"><span class="pre">HeuristicalStorageReservation.last_reserved_topology</span></code></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#torchrec.distributed.planner.proposers.GreedyProposer"><code class="docutils literal notranslate"><span class="pre">GreedyProposer</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torchrec.distributed.planner.proposers.GreedyProposer.feedback"><code class="docutils literal notranslate"><span class="pre">GreedyProposer.feedback()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torchrec.distributed.planner.proposers.GreedyProposer.load"><code class="docutils literal notranslate"><span class="pre">GreedyProposer.load()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torchrec.distributed.planner.proposers.GreedyProposer.propose"><code class="docutils literal notranslate"><span class="pre">GreedyProposer.propose()</span></code></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#torchrec.distributed.planner.shard_estimators.EmbeddingPerfEstimator"><code class="docutils literal notranslate"><span class="pre">EmbeddingPerfEstimator</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torchrec.distributed.planner.shard_estimators.EmbeddingPerfEstimator.estimate"><code class="docutils literal notranslate"><span class="pre">EmbeddingPerfEstimator.estimate()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torchrec.distributed.planner.shard_estimators.EmbeddingPerfEstimator.perf_func_emb_wall_time"><code class="docutils literal notranslate"><span class="pre">EmbeddingPerfEstimator.perf_func_emb_wall_time()</span></code></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#torchrec.distributed.planner.shard_estimators.EmbeddingStorageEstimator"><code class="docutils literal notranslate"><span class="pre">EmbeddingStorageEstimator</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torchrec.distributed.planner.shard_estimators.EmbeddingStorageEstimator.estimate"><code class="docutils literal notranslate"><span class="pre">EmbeddingStorageEstimator.estimate()</span></code></a></li>
</ul>
</li>
</ul>
  </nav></div>
    
       <div class="sidebar-secondary-item">

  
  <div class="tocsection editthispage">
    <a href="https://github.com/meta-pytorch/torchrec/edit/main/docs/source/planner-api-reference.rst">
      <i class="fa-solid fa-pencil"></i>
      
      
        
          Edit on GitHub
        
      
    </a>
  </div>
</div>
    
       <div class="sidebar-secondary-item">
    <div class="tocsection sourcelink">
      <a href="_sources/planner-api-reference.rst.txt">
        <i class="fa-solid fa-file-lines"></i> Show Source
      </a>
    </div>
</div>
    




</div>
</div>
              
            
          </div>
          <footer class="bd-footer-content">
            
          </footer>
        
      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  


<style>
.site-footer {
    padding: 20px 40px;
    height: 60px !important;
}

@media screen and (min-width: 768px) {
    .site-footer {
        padding: 20px 40px;
    }
}

.site-footer .privacy-policy {
    border-top: none;
    margin-top: 0px;
}

.site-footer .privacy-policy .copyright {
    padding-top: 0;
}
</style>


<footer class="site-footer">

    <div class="privacy-policy">
      <div class="copyright">
      
        <p>
           Copyright © 2025 Meta Platforms, Inc
        </p>
        
      </div>
    </div>


  </div>
</footer>

<div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="_static/img/pytorch-x.svg">
  </div>
</div>
  
  <footer class="bd-footer">
<div class="bd-footer__inner bd-page-width">
  
    <div class="footer-items__start">
      
        <div class="footer-item">

  <p class="copyright">
    
      © Copyright Meta Platforms, Inc.
      <br/>
    
  </p>
</div>
      
        <div class="footer-item">

  <p class="sphinx-version">
    Created using <a href="https://www.sphinx-doc.org/">Sphinx</a> 7.2.6.
    <br/>
  </p>
</div>
      
    </div>
  
  
  
    <div class="footer-items__end">
      
        <div class="footer-item">
<p class="theme-version">
  Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.15.4.
</p></div>
      
    </div>
  
</div>

  </footer>
  <script type="application/ld+json">
    {
       "@context": "https://schema.org",
       "@type": "Article",
       "name": "Planner",
       "headline": "Planner",
       "description": "PyTorch Documentation. Explore PyTorch, an open-source machine learning library that accelerates the path from research prototyping to production deployment. Discover tutorials, API references, and guides to help you build and deploy deep learning models efficiently.",
       "url": "/planner-api-reference.html",
       "articleBody": "Planner# The TorchRec Planner is responsible for determining the most performant, balanced sharding plan for distributed training and inference. The main API for generating a sharding plan is EmbeddingShardingPlanner.plan class torchrec.distributed.types.ShardingPlan(plan: Dict[str, ModuleShardingPlan])# Representation of sharding plan. This uses the FQN of the larger wrapped model (i.e the model that is wrapped using DistributedModelParallel) EmbeddingModuleShardingPlan should be used when TorchRec composability is desired. plan# dict keyed by module path of dict of parameter sharding specs keyed by parameter name. Type: Dict[str, EmbeddingModuleShardingPlan] get_plan_for_module(module_path: str) \u2192 ModuleShardingPlan | None# Parameters: module_path (str) \u2013 Returns: dict of parameter sharding specs keyed by parameter name. None if sharding specs do not exist for given module_path. Return type: Optional[ModuleShardingPlan] class torchrec.distributed.planner.planners.EmbeddingShardingPlanner(topology: Topology | None = None, batch_size: int | None = None, enumerator: Enumerator | None = None, storage_reservation: StorageReservation | None = None, proposer: Proposer | List[Proposer] | None = None, partitioner: Partitioner | None = None, performance_model: PerfModel | None = None, stats: Stats | List[Stats] | None = None, constraints: Dict[str, ParameterConstraints] | None = None, debug: bool = True, callbacks: List[Callable[[List[ShardingOption]], List[ShardingOption]]] | None = None, timeout_seconds: int | None = None, plan_loader: PlanLoader | None = None)# Provides an optimized sharding plan for a given module with shardable parameters according to the provided sharders, topology, and constraints. Parameters: topology (Optional[Topology]) \u2013 the topology of the current process group. batch_size (Optional[int]) \u2013 the batch size of the model. enumerator (Optional[Enumerator]) \u2013 the enumerator to use storage_reservation (Optional[StorageReservation]) \u2013 the storage reservation to use proposer (Optional[Union[Proposer, List[Proposer]]]) \u2013 the proposer(s) to use partitioner (Optional[Partitioner]) \u2013 the partitioner to use performance_model (Optional[PerfModel]) \u2013 the performance model to use stats (Optional[Union[Stats, List[Stats]]]) \u2013 the stats to use constraints (Optional[Dict[str, ParameterConstraints]]) \u2013 per table constraints for sharding. debug (bool) \u2013 whether to print debug information. Example: ebc = EmbeddingBagCollection(tables=eb_configs, device=torch.device(\"meta\")) planner = EmbeddingShardingPlanner() plan = planner.plan( module=ebc, sharders=[EmbeddingBagCollectionSharder()], ) collective_plan(module: Module, sharders: List[ModuleSharder[Module]] | None = None, pg: ProcessGroup | None = None) \u2192 ShardingPlan# Call self.plan(\u2026) on rank 0 and broadcast Parameters: module (nn.Module) \u2013 the module to shard. sharders (Optional[List[ModuleSharder[nn.Module]]]) \u2013 the sharders to use for sharding pg (Optional[dist.ProcessGroup]) \u2013 the process group to use for collective operations Returns: the sharding plan for the module. Return type: ShardingPlan plan(module: Module, sharders: List[ModuleSharder[Module]]) \u2192 ShardingPlan# Provides an optimized sharding plan for a given module with shardable parameters according to the provided sharders, topology, and constraints. Parameters: module (nn.Module) \u2013 the module to shard. sharders (List[ModuleSharder[nn.Module]]) \u2013 the sharders to use for sharding. Returns: the sharding plan for the module. Return type: ShardingPlan class torchrec.distributed.planner.enumerators.EmbeddingEnumerator(topology: Topology, batch_size: int, constraints: Dict[str, ParameterConstraints] | None = None, estimator: ShardEstimator | List[ShardEstimator] | None = None, use_exact_enumerate_order: bool | None = False)# Generates embedding sharding options for given nn.Module, considering user provided constraints. Parameters: topology (Topology) \u2013 device topology. batch_size (int) \u2013 batch size. constraints (Optional[Dict[str, ParameterConstraints]]) \u2013 dict of parameter names to provided ParameterConstraints. estimator (Optional[Union[ShardEstimator, List[ShardEstimator]]]) \u2013 shard performance estimators. use_exact_enumerate_order (bool) \u2013 whether to enumerate shardable parameters in the exact name_children enumeration order enumerate(module: Module, sharders: List[ModuleSharder[Module]]) \u2192 List[ShardingOption]# Generates relevant sharding options given module and sharders. Parameters: module (nn.Module) \u2013 module to be sharded. sharders (List[ModuleSharder[nn.Module]]) \u2013 provided sharders for module. Returns: valid sharding options with values populated. Return type: List[ShardingOption] populate_estimates(sharding_options: List[ShardingOption]) \u2192 None# See class description. class torchrec.distributed.planner.partitioners.GreedyPerfPartitioner(sort_by: SortBy = SortBy.STORAGE, balance_modules: bool = False)# Greedy Partitioner. Parameters: sort_by (SortBy) \u2013 Sort sharding options by storage or perf in descending order (i.e., large tables will be placed first). balance_modules (bool) \u2013 Whether to sort by modules first, where smaller modules will be sorted first. In effect, this will place tables in each module in a balanced way. partition(proposal: List[ShardingOption], storage_constraint: Topology, hbm_per_device: int | None = None) \u2192 List[ShardingOption]# Places sharding options on topology based on each sharding option\u2019s partition_by attribute. The topology, storage, and perfs are updated at the end of the placement. Parameters: proposal (List[ShardingOption]) \u2013 list of populated sharding options. storage_constraint (Topology) \u2013 device topology. Returns: list of sharding options for selected plan. Return type: List[ShardingOption] Example: sharding_options = [ ShardingOption(partition_by=\"uniform\", shards=[ Shards(storage=1, perf=1), Shards(storage=1, perf=1), ]), ShardingOption(partition_by=\"uniform\", shards=[ Shards(storage=2, perf=2), Shards(storage=2, perf=2), ]), ShardingOption(partition_by=\"device\", shards=[ Shards(storage=3, perf=3), Shards(storage=3, perf=3), ]) ShardingOption(partition_by=\"device\", shards=[ Shards(storage=4, perf=4), Shards(storage=4, perf=4), ]), ] topology = Topology(world_size=2) # First [sharding_options[0] and sharding_options[1]] will be placed on the # topology with the uniform strategy, resulting in topology.devices[0].perf.total = (1,2) topology.devices[1].perf.total = (1,2) # Finally sharding_options[2] and sharding_options[3]] will be placed on the # topology with the device strategy (see docstring of `partition_by_device` for # more details). topology.devices[0].perf.total = (1,2) + (3,4) topology.devices[1].perf.total = (1,2) + (3,4) # The topology updates are done after the end of all the placements (the other # in the example is just for clarity). class torchrec.distributed.planner.storage_reservations.HeuristicalStorageReservation(percentage: float, parameter_multiplier: float = 6.0, dense_tensor_estimate: int | None = None)# Reserves storage for model to be sharded with heuristical calculation. The storage reservation is comprised of dense tensor storage, KJT storage, and an extra percentage of total storage. Parameters: percentage (float) \u2013 extra storage percent to reserve that acts as a margin of error beyond heuristic calculation of storage. parameter_multiplier (float) \u2013 heuristic multiplier for total parameter storage. dense_tensor_estimate (Optional[int]) \u2013 storage estimate for dense tensors, uses default heuristic estimate if not provided. property last_reserved_topology: Topology | None# Cached value of the most recent output from the reserve() method. class torchrec.distributed.planner.proposers.GreedyProposer(use_depth: bool = True, threshold: int | None = None)# Proposes sharding plans in greedy fashion. Sorts sharding options for each shardable parameter by perf. On each iteration, finds parameter with largest current storage usage and tries its next sharding option. Parameters: use_depth (bool) \u2013 When enabled, sharding_options of a fqn are sorted based on max(shard.perf.total), otherwise sharding_options are sorted by sum(shard.perf.total). threshold (Optional[int]) \u2013 Threshold for early stopping. When specified, the proposer stops proposing when the proposals have consecutive worse perf_rating than best_perf_rating. feedback(partitionable: bool, plan: List[ShardingOption] | None = None, perf_rating: float | None = None, storage_constraint: Topology | None = None) \u2192 None# Provide feedback to proposer. Parameters: partitionable (bool) \u2013 whether the plan is partitionable. plan (Optional[List[ShardingOption]]) \u2013 plan to provide feedback on. perf_rating (Optional[float]) \u2013 performance rating of the plan. storage_constraint (Optional[Topology]) \u2013 storage constraint of the plan. load(search_space: List[ShardingOption], enumerator: Enumerator | None = None) \u2192 None# Load search space into proposer. Parameters: search_space (List[ShardingOption]) \u2013 search space to load. enumerator (Enumerator) \u2013 enumerator used to generate search space. propose() \u2192 List[ShardingOption] | None# Propose a sharding plan. Returns: proposed plan. Return type: Optional[List[ShardingOption]] class torchrec.distributed.planner.shard_estimators.EmbeddingPerfEstimator(topology: Topology, constraints: Dict[str, ParameterConstraints] | None = None, is_inference: bool = False)# Embedding Wall Time Perf Estimator. This estimator estimates the wall time of a given sharding option. Parameters: topology (Topology) \u2013 device topology. constraints (Optional[Dict[str, ParameterConstraints]]) \u2013 parameter constraints. is_inference (bool) \u2013 whether or not the estimator is used for inference. estimate(sharding_options: List[ShardingOption], sharder_map: Dict[str, ModuleSharder[Module]] | None = None) \u2192 None# Estimates the wall time of a given sharding option. Parameters: sharding_options (List[ShardingOption]) \u2013 list of sharding options. sharder_map (Optional[Dict[str, ModuleSharder[nn.Module]]]) \u2013 sharder map. classmethod perf_func_emb_wall_time(shard_sizes: List[List[int]], compute_kernel: str, compute_device: str, sharding_type: str, batch_sizes: List[int], world_size: int, local_world_size: int, input_lengths: List[float], input_data_type_size: float, table_data_type_size: float, output_data_type_size: float, fwd_a2a_comm_data_type_size: float, bwd_a2a_comm_data_type_size: float, fwd_sr_comm_data_type_size: float, bwd_sr_comm_data_type_size: float, num_poolings: List[float], hbm_mem_bw: float, ddr_mem_bw: float, hbm_to_ddr_mem_bw: float, comms_bandwidths: GeneralizedCommsBandwidth, bwd_compute_multiplier: float, weighted_feature_bwd_compute_multiplier: float, is_pooled: bool, is_weighted: bool = False, caching_ratio: float | None = None, is_inference: bool = False, prefetch_pipeline: bool = False, expected_cache_fetches: float = 0, uneven_sharding_perf_multiplier: float = 1.0) \u2192 List[Perf]# Attempts to model perfs as a function of relative wall times. Parameters: shard_sizes (List[List[int]]) \u2013 the list of (local_rows, local_cols) of each shard. compute_kernel (str) \u2013 compute kernel. compute_device (str) \u2013 compute device. sharding_type (str) \u2013 tw, rw, cw, twrw, dp. batch_sizes (List[int]) \u2013 batch size for each input feature. world_size (int) \u2013 the number of devices for all hosts. local_world_size (int) \u2013 the number of the device for each host. input_lengths (List[float]) \u2013 the list of the average number of lookups of each input query feature. Also referred to as pooling_mean, and it\u2019s equal to the pooling_factor * num_pooling. input_data_type_size (float) \u2013 the data type size of the distributed data_parallel input. table_data_type_size (float) \u2013 the data type size of the table. output_data_type_size (float) \u2013 the data type size of the output embeddings. fwd_comm_data_type_size (float) \u2013 the data type size of the distributed data_parallel input during forward communication. bwd_comm_data_type_size (float) \u2013 the data type size of the distributed data_parallel input during backward communication. num_poolings (List[float]) \u2013 number of poolings per sample, typically 1.0 for non-EBF use cases. In EBF use cases, this is the number of events per sample. hbm_mem_bw (float) \u2013 the bandwidth of the device HBM. ddr_mem_bw (float) \u2013 the bandwidth of the system DDR memory. hbm_to_ddr_bw (float) \u2013 the bandwidth between device HBM and system DDR. intra_host_bw (float) \u2013 the bandwidth within a single host like multiple threads. inter_host_bw (float) \u2013 the bandwidth between two hosts like multiple machines. is_pooled (bool) \u2013 True if embedding output is pooled (ie. EmbeddingBag), False if unpooled/sequential (ie. Embedding). is_weighted (bool = False) \u2013 if the module is an EBC and is weighted, typically signifying an id score list feature. is_inference (bool = False) \u2013 if planning for inference. caching_ratio (Optional[float] = None) \u2013 cache ratio to determine the bandwidth of device. prefetch_pipeline (bool = False) \u2013 whether prefetch pipeline is enabled. expected_cache_fetches (float) \u2013 number of expected cache fetches across global batch uneven_sharding_perf_multiplier (float = 1.0) \u2013 multiplier to account for uneven sharding perf Returns: the list of perf for each shard. Return type: List[float] class torchrec.distributed.planner.shard_estimators.EmbeddingStorageEstimator(topology: Topology, constraints: Dict[str, ParameterConstraints] | None = None, pipeline_type: PipelineType = PipelineType.NONE, run_embedding_at_peak_memory: bool = False, is_inference: bool = False)# Embedding Storage Usage Estimator Parameters: topology (Topology) \u2013 device topology. constraints (Optional[Dict[str, ParameterConstraints]]) \u2013 parameter constraints. pipeline_type (PipelineType) \u2013 The type of pipeline, if any. Will determine the input replication factor during memory estimation. run_embedding_at_peak_memory (bool) \u2013 If the embedding fwd/bwd will be execute when HBM usage is at peak. When set to TRUE, any temporary memory allocation during embedding forward/backward, as long as output sizes before output_dist will be counted towards HBM storage cost. Otherwise they won\u2019t since they\u2019ll be \u201chidden\u201d by the real memory peak. Only take effect if pipeline_type is set for backward compatibility (not affecting models using old pipeline-agnostic formula) Default to false because this is typically false for RecSys since memory peak happens at the end of dense forwrad / beginning of dense backward instead. is_inference (bool) \u2013 If the model is inference model. Default to False. estimate(sharding_options: List[ShardingOption], sharder_map: Dict[str, ModuleSharder[Module]] | None = None) \u2192 None# Estimate the storage cost of each sharding option. Parameters: sharding_options (List[ShardingOption]) \u2013 list of sharding options. sharder_map (Optional[Dict[str, ModuleSharder[nn.Module]]]) \u2013 map from module type to sharder.",
       "author": {
         "@type": "Organization",
         "name": "PyTorch Contributors",
         "url": "https://pytorch.org"
       },
       "image": "https://pytorch.org/docs/stable/_static/img/pytorch_seo.png",
       "mainEntityOfPage": {
         "@type": "WebPage",
         "@id": "/planner-api-reference.html"
       },
       "datePublished": "2023-01-01T00:00:00Z",
       "dateModified": "2023-01-01T00:00:00Z"
     }
 </script>
  <script>
    // Tutorials Call to action event tracking
    $("[data-behavior='call-to-action-event']").on('click', function () {
      fbq('trackCustom', "Download", {
        tutorialTitle: $('h1:first').text(),
        downloadLink: this.href,
        tutorialLink: window.location.href,
        downloadTitle: $(this).attr("data-response")
      });
      if (typeof gtag === 'function') {
        gtag('event', 'click', {
          'event_category': $(this).attr("data-response"),
          'event_label': $("h1").first().text(),
          'tutorial_link': window.location.href
        });
      }
    });
  </script>
  
  </body>
</html>